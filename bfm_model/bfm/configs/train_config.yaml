model:
  T: 2   # Time steps
  surface_vars: ["t2m", "msl", "slt", "z", "u10", "v10", "lsm"] # Surface variables
  edaphic_vars: ["swvl1", "swvl2", "stl1", "stl2"] # Edaphic variables
  atmos_vars: ["z", "t", "u", "v", "q"]  # Atmospheric variables
  climate_vars: ["smlt", "tp", "csfr", "avg_sdswrf", "avg_snswrf", "avg_snlwrf",
                  "avg_tprate", "avg_sdswrfcs", "sd", "t2m", "d2m"] # Climate variables
  species_vars: ["1340361", "1340503", "1536449", "1898286", "1920506", "2430567",
                       "2431885", "2433433", "2434779", "2435240", "2435261", "2437394",
                       "2441454", "2473958", "2491534", "2891770", "3034825", "4408498",
                        "5218786", "5219073", "5219173", "5219219", "5844449", "8002952",
                        "8077224", "8894817", "8909809", "9809229"]  # Species variables
  vegetation_vars: ["NDVI"] # Vegetation variables
  land_vars: ["Land"] # Land variables
  agriculture_vars: ["Agriculture", "Arable", "Cropland"] # Agriculture variables
  forest_vars: ["Forest"] # Forest variables
  redlist_vars: ["RLI"] # Red List variables
  misc_vars: ["avg_slhtf", "avg_pevr"] # Misc variables
  H: 160 #152  # Height
  W: 280 #320  # Width
  num_latent_tokens: 8 #TODO Check consistensy in name and value
  backbone: "swin" # mvit
  patch_size: 4 #2 # Size of spatial patches.
  embed_dim: 512 #512 # Embedding dimension.
  num_heads: 16 # Number of attention heads.
  head_dim: 64 # Dimension of each attention head.
  depth: 6 #6 # Number of transformer layers.
  swin_backbone_size: "medium" # Can be "medium" or "large"

data:
  atmos_levels: [1000, 925, 850, 700, 600, 500, 400, 300, 250, 200, 150, 100, 50]
  species_number: 28 # available species
  data_path: /projects/prjs1134/data/projects/biodt/storage/final_dataset_monthly/train
  test_data_path: /projects/prjs1134/data/projects/biodt/storage/final_dataset_monthly/test
  land_mask_path: batch_statistics/europe_Land_2020_grid.pkl

  scaling:
    enabled: True
    stats_path: /projects/prjs1134/data/projects/biodt/storage/monthly_batches/statistics/monthly_batches_stats_splitted_channels.json
    mode: normalize # normalize (mean and std), standardize (min-max)

training:
  batch_size: 3
  workers: 15
  epochs: 1000
  accelerator: cuda
  precision: bf16-mixed
  precision_in: "high" # ["medium", "high", "highest"] https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
  devices: [0, 1]
  strategy: fsdp #fsdp #ddp
  num_nodes: 1
  gradient_clip: 1.0
  log_steps: 1
  lr: 0.00003  #5e-4
  wd: 0.000003 #5e-6
  checkpoint_path: #/home/atrantas/bfm-model/outputs/2025-06-03/14-27-12/checkpoints/epoch=340-val_loss=0.00555.ckpt #/home/atrantas/bfm-model/outputs/2025-06-02/10-17-22/checkpoints/epoch=112-val_loss=0.00970.ckpt #/home/atrantas/bfm-model/outputs/2025-05-25/12-39-21/checkpoints/epoch=195-val_loss=0.00404.ckpt #/home/atrantas/bfm-model/outputs/2025-05-24/19-55-32/checkpoints/epoch=158-val_loss=0.00413.ckpt #/home/atrantas/bfm-model/outputs/2025-05-24/15-29-50/checkpoints/epoch=36-val_loss=0.00565.ckpt # Add your checkpoint (.pt) path to continue the pre-training
  checkpoint_every: 31
  eval_every: 30
  td_learning: False
  use_mask: "no" # "no", "fully", or "partially"
  partially_masked_groups: ["species_variables"]

general:
  do_eval: True

mlflow:
  port: 8082 # change here port if you get [Errno 98] Address already in use

evaluation:
  rollout_data: /home/atrantas/bfm-model/rollout_monthly #/home/thanasis.trantas/git_projects/bfm-model/data_small/rollout_monthly # Need to have 2 batch samples for the rollout to work with
  checkpoint_path: /projects/prjs1134/data/projects/biodt/storage/weights/epoch=268-val_loss=0.00493.ckpt # THE BEST PERFORMING /home/thanasis.trantas/git_projects/bfm-model/weights/epoch=268-val_loss=0.00493.ckpt #/home/atrantas/bfm-model/outputs/2025-05-25/12-36-44/checkpoints/epoch=65-val_loss=36.95078.ckpt #/home/atrantas/bfm-model/outputs/2025-05-24/19-55-32/checkpoints/epoch=158-val_loss=0.00413.ckpt #/home/atrantas/bfm-model/outputs/2025-05-24/15-29-50/checkpoints/epoch=26-val_loss=0.00631.ckpt
  batch_size: 1
  test_device: "cuda:0"

finetune:
  rollout_steps: 20
  epochs: 500
  batch_size: 1
  eval_every: 100
  checkpoint_every: 101
  lr: 0.00003  #5e-4
  wd: 0.000003 #5e-6
  td_learning: False
  checkpoint_path: /home/atrantas/bfm-model/outputs/2025-06-03/14-27-12/checkpoints/epoch=340-val_loss=0.00555.ckpt # Add your checkpoint (.pt) path to start fine-tuning
  prediction: False
  peft_mode: "all"
  mode: "short" # weekly # monthly
  rank: 512 # LoRA | 256 VeRA
  peft_dropout: 0.0
  use_lora: False
  use_vera: True
  lora_alpha: 8
  d_initial: 0.1

model_swin_backbone:
  large:  # smaller batch size advised, when using one h100
    encoder_depths: [2,2,2]
    encoder_num_heads: [8,16,32]
    decoder_depths: [2,2,2]
    decoder_num_heads: [32,16,8]
    window_size: [1,4,5]
    mlp_ratio: 4.0
    qkv_bias: True
    drop_rate: 0.0
    attn_drop_rate: 0.0
    drop_path_rate: 0.1
    use_lora: False
  medium:
    encoder_depths: [2,2]
    encoder_num_heads: [8,16]
    decoder_depths: [2,2]
    decoder_num_heads: [16,8]
    window_size: [1,1,1] # [1, 4, 5]
    mlp_ratio: 4.0
    qkv_bias: True
    drop_rate: 0.0
    attn_drop_rate: 0.0
    drop_path_rate: 0.1
    use_lora: False
