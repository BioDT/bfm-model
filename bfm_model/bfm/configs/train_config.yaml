model:
  T: 2   # Time steps
  surface_vars: ["t2m", "msl", "slt", "z", "u10", "v10", "lsm"] # Surface variables
  edaphic_vars: ["swvl1", "swvl2", "stl1", "stl2"] # Edaphic variables
  atmos_vars: ["z", "t", "u", "v", "q"]  # Atmospheric variables
  climate_vars: ["smlt", "tp", "csfr", "avg_sdswrf", "avg_snswrf", "avg_snlwrf", 
                  "avg_tprate", "avg_sdswrfcs", "sd", "t2m", "d2m"] # Climate variables
  species_vars: ["1340361", "1340503", "1536449", "1898286", "1920506", "2430567",
                       "2431885", "2433433", "2434779", "2435240", "2435261", "2437394",
                       "2441454", "2473958", "2491534", "2891770", "3034825", "4408498",
                        "5218786", "5219073", "5219173", "5219219", "5844449", "8002952",
                        "8077224", "8894817", "8909809", "9809229"]  # Species variables
  vegetation_vars: ["NDVI"] # Vegetation variables
  land_vars: ["Land"] # Land variables
  agriculture_vars: ["Agriculture", "Arable", "Cropland"] # Agriculture variables
  forest_vars: ["Forest"] # Forest variables
  redlist_vars: ["RLI"] # Red List variables
  misc_vars: ["avg_slhtf", "avg_pevr"] # Misc variables
  H: 160 #152  # Height
  W: 280 #320  # Width
  num_latent_tokens: 8 #TODO Check consistensy in name and value
  backbone: "swin" # mvit
  patch_size: 4 # Size of spatial patches.
  embed_dim: 256 # Embedding dimension.
  num_heads: 16 # Number of attention heads.
  head_dim: 64 # Dimension of each attention head.
  depth: 3 # Number of transformer layers.

data:
  atmos_levels: [1000, 925, 850, 700, 600, 500, 400, 300, 250, 200, 150, 100, 50]
  species_number: 28 # available species
  data_path: /projects/prjs1134/data/projects/biodt/storage/final_dataset_monthly/train
  test_data_path: /projects/prjs1134/data/projects/biodt/storage/final_dataset_monthly/test
  
  scaling:
    enabled: True
    stats_path: /projects/prjs1134/data/projects/biodt/storage/monthly_batches/statistics/monthly_batches_stats_splitted_channels.json
    mode: normalize # normalize (mean and std), standardize (min-max)

training:
  batch_size: 6
  workers: 15
  epochs: 100
  accelerator: cuda
  precision: bf16-mixed
  precision_in: "high" # ["medium", "high", "highest"] https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
  devices: [0]
  strategy: fsdp #fsdp #ddp
  num_nodes: 1
  gradient_clip: 1.0
  log_steps: 1
  lr: 0.003  #5e-4
  wd: 0.001 #5e-6
  checkpoint_path: # Add your checkpoint (.pt) path to continue the pre-training
  checkpoint_every: 21
  eval_every: 20
  td_learning: False

general:
  do_eval: True

mlflow:
  port: 8082 # change here port if you get [Errno 98] Address already in use

evaluation:
  test_data:  /projects/prjs1134/data/projects/biodt/storage/final_dataset_monthly/test
  checkpoint_path: /home/atrantas/bfm-model/outputs/2025-05-23/17-20-38-gettingthere/checkpoints/epoch=20-train_loss=0.03.ckpt
  batch_size: 1
  test_device: "cuda:0"

finetune:
  rollout_steps: 2
  epochs: 1
  batch_size: 1
  eval_every: 200
  checkpoint_every: 100
  lr: 0.00005  #5e-4
  wd: 0.0001 #5e-6
  td_learning: False
  checkpoint_path: # Add your checkpoint (.pt) path to start fine-tuning
  prediction: False
  peft_mode: "all"
  mode: "short" # weekly # monthly
  rank: 8 # LoRA | 256 VeRA
  peft_dropout: 0.0
  use_lora: False
  use_vera: True
  lora_alpha: 8
  d_initial: 0.1