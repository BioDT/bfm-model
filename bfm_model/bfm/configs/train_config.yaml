model:
  T: 2   # Time steps
  V_surf: 2 # Number of surface variables
  V_atmos: 2 # Number of atmospheric variables
  C: 3  # Number of atmospheric levels
  V_spec: 1 # Number of extinct_species variables
  V_land: 2 # Number of land variables
  V_agri: 4 # Number of agriculture variables
  V_forest: 1 # Number of forest variables
  H: 152  # Height
  W: 320  # Width
  num_latent_tokens: 8 #TODO Check consistensy in name and value
  backbone: "swin" # mvit
  patch_size: 4 # Size of spatial patches.
  embed_dim: 128 # Embedding dimension.
  num_heads: 16 # Number of attention heads.
  head_dim: 64 # Dimension of each attention head.
  depth: 3 # Number of transformer layers.

data:
  atmos_levels: [50, 500, 1000]
  species_number: 22 # [0-22] available species
  data_path: /projects/prjs1134/data/projects/biodt/storage/final_dataset/train
  # data_path: /home/thanasis.trantas/git_projects/bfm-model/data_small # hinton
  test_data_path: /projects/prjs1134/data/projects/biodt/storage/final_dataset/test
  # test_data_path: /home/thanasis.trantas/git_projects/bfm-model/data_small # hinton
  scaling:
    enabled: True
    stats_path: batch_statistics/statistics.json # #/projects/prjs1134/data/projects/biodt/storage/batches_2025_02_17/statistics.json # snellius
    # stats_path: /data/projects/biodt/storage/batches/statistics.json # hinton
    mode: normalize # normalize (mean and std), standardize (min-max)
#TODO Checkpointing path: /scratch-shared/<username>

training:
  batch_size: 4
  workers: 15
  epochs: 50
  accelerator: cuda
  precision: bf16-mixed
  precision_in: "high" # ["medium", "high", "highest"] https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
  devices: 4
  strategy: fsdp #fsdp #ddp
  num_nodes: 2
  gradient_clip: 1.0
  log_steps: 1
  lr: 0.00001  #5e-4
  wd: 0.00001 #5e-6
  checkpoint_path: /home/atrantas/bfm-model/outputs/2025-04-16/21-14-59/checkpoints/epoch=23-train_loss=0.02.ckpt #"/gpfs/home2/damian/projects/bfm-model/outputs"
  # resume_from_checkpoint: False
  checkpoint_every: 200
  eval_every: 200
  td_learning: True

general:
  do_eval: True

mlflow:
  port: 8082 # change here port if you get [Errno 98] Address already in use

evaluation:
  checkpoint_path: #"/gpfs/scratch1/shared/damian/bflm/outputs/outputs/2025-04-09/23-06-37/checkpoints/last.ckpt" #/home/atrantas/bfm-model/outputs/2025-04-12/13-25-36/checkpoints/epoch=02-train_loss=0.04.ckpt #
  batch_size: 1
  test_device: "cuda:0"

finetune:
  rollout_steps: 16
  epochs: 20
  batch_size: 4
  eval_every: 1000
  checkpoint_every: 100
  lr: 0.00005  #5e-4
  wd: 0.0001 #5e-6
  td_learning: False
  lora_mode: "all"
  mode: "short" # weekly # monthly
  checkpoint_path: /home/atrantas/bfm-model/outputs/2025-04-16/21-14-59/checkpoints/epoch=23-train_loss=0.02.ckpt #"/gpfs/home2/damian/projects/bfm-model/outputs"