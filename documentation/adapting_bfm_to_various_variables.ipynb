{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <span style=\"color: orange;\">Adapting the Biodiversity Foundation Model (BFM) for different data formats</span>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <span style=\"color: orange;\">Introduction</span>\n",
                "The Biodiversity Foundation Model (BFM) is a flexible architecture designed to handle complex environmental data - from extinction indexes to atmospheric data. This notebook aims to demonstrate how one could adapt this architecture for different types of data and use cases.\n",
                "\n",
                "We will (try to) cover:\n",
                "- Understanding the BFM architecture;\n",
                "- Key components that make BFM adaptable;\n",
                "- Step-by-step guide to adapting BFM for new data formats;\n",
                "- Practical example with Air Quality data;\n",
                "\n",
                "Hope you'll get a better understanding of what this project is all about and what can be done with it (:  \n",
                "Cheers,  \n",
                "Sebastian Gribincea"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <span style=\"color: orange;\">BFM Architecture Overview</span>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The BFM consists of three main components:\n",
                "1. **Encoder**: Processes various input data types and formats, and creates a unified representation of shape [B, L, D] (check @encoder.py in )\n",
                "2. **Backbone**: Handles temporal and spatial relationships (using either MViT or Swin Transformer)\n",
                "3. **Decoder**: Generates predictions in the required output format\n",
                "\n",
                "Key features that make BFM adaptable:\n",
                "- Highly modular design, both within and between BFM building blocks;\n",
                "- Flexible input handling (thanks to Perceiver IO);\n",
                "- Lots of configurable hyperparameters.  \n",
                "<span style=\"color: orange;\">**Careful on this one!**</span>, multiple assertion errors were added across the code files, to ensure we are properly handling the data);  \n",
                "- Adaptable position encodings;\n",
                "- ..and many others"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Literal\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "# simplified BFM structure, for a smoother start\n",
                "class SimplifiedBFM(nn.Module):\n",
                "    def __init__(\n",
                "        self,\n",
                "        input_vars: dict,\n",
                "        embed_dim: int = 1024,\n",
                "        num_latent_tokens: int = 8,\n",
                "        backbone_type: Literal[\"swin\", \"mvit\"] = \"mvit\",\n",
                "        **kwargs\n",
                "    ):\n",
                "        super().__init__()\n",
                "        # Three main components\n",
                "        self.encoder = self._create_encoder(input_vars, embed_dim, **kwargs)\n",
                "        self.backbone = self._create_backbone(backbone_type, embed_dim, **kwargs)\n",
                "        self.decoder = self._create_decoder(input_vars, embed_dim, **kwargs)\n",
                "    \n",
                "    def forward(self, x, metadata):\n",
                "        # encode\n",
                "        encoded = self.encoder(x)\n",
                "        ### any other processing necessary to the embeddings before passing them to the backbone ###\n",
                "        # process\n",
                "        processed = self.backbone(encoded) # should have the same [B, L, D] shape as the encoded (and maybe slightly modified) embeddings\n",
                "        # decode\n",
                "        output = self.decoder(processed)\n",
                "        return output"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <span style=\"color: orange;\">Why Adapt BFM?</span>\n",
                "\n",
                "The BFM was originally designed for biodiversity data (and any related to variables, like atmospheric ones), but its architecture makes it potentially suitable for various environmental and time-series prediction tasks. Benefits of adaptation include:\n",
                "\n",
                "1. **Reusable components**: core mechanisms like Perceiver IO and temporal processing can be preserved;\n",
                "2. **Proven architecture**: Built on tested transformer-based approaches;\n",
                "3. **Flexible I/O**: Can handle various data formats and tasks (i.e., modality-/task-agnosticity inherent by design);\n",
                "4. **Scalable design**: Works with different data dimensions and temporal scales;\n",
                "\n",
                "In the following sections, we'll explore how to adapt this architecture for different use cases, using the Air Quality Forecasting Model (AQFM) as our examplar."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <span style=\"color: orange;\">Original BFM architecture analysis</span>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this section, we will analyze the original components of the BFM. Understanding these components is, as you might've already guessed, rather important for adapting the architecture to different data formats.\n",
                "\n",
                "As mentioned, we have these key components:\n",
                "1. **Encoder**;\n",
                "2. **Decoder**;\n",
                "3. **Backbone**.\n",
                "\n",
                "Before adapting the BFM, we need to understand its core components. Let's analyze a simplified version that handles just two types of variables:\n",
                "- Surface variables (2D spatial data)\n",
                "- Atmospheric variables (3D data with pressure levels)\n",
                "\n",
                "This simplification will help us understand the essential patterns without getting lost in complexity.\n",
                "\n",
                "Let's start by examining the encoder in detail!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": [
                "# all the necessary imports\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch import nn\n",
                "from dataclasses import dataclass\n",
                "from typing import Dict, List, Tuple\n",
                "from datetime import datetime, timedelta\n",
                "from einops import rearrange, repeat\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": [
                "# we will use a simplified Batch structure, again, to not have a too long notebook\n",
                "@dataclass\n",
                "class SimpleBatchMetadata:\n",
                "    \"\"\"Simplified metadata structure\"\"\"\n",
                "    latitudes: torch.Tensor\n",
                "    longitudes: torch.Tensor\n",
                "    pressure_levels: tuple[int, ...]\n",
                "    timestamp: tuple[datetime, datetime]\n",
                "\n",
                "@dataclass\n",
                "class SimpleBatch:\n",
                "    \"\"\"Simplified batch structure with some of the used variable types\"\"\"\n",
                "    surface_variables: dict[str, torch.Tensor]\n",
                "    atmospheric_variables: dict[str, torch.Tensor]\n",
                "    batch_metadata: SimpleBatchMetadata"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <span style=\"color: orange;\">BFM Encoder Analysis</span>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The encoder is responsible for converting various physical variables into a unified latent representation.\n",
                "\n",
                "Key responsibilities:\n",
                "1. Processing multiple variable types (surface, atmospheric, agriculture/land etc., you name it);\n",
                "2. Organizing spatial data into patches;\n",
                "3. Creating embeddings that capture variable relationships;\n",
                "4. Enriching the data with temporal and spatial context.\n",
                "\n",
                "Key design patterns:\n",
                "- Separate embedding layers for each variable type;\n",
                "- Patch-based processing for spatial data;\n",
                "- Position and time encodings;\n",
                "- Perceiver IO for unified representation.\n",
                "\n",
                "Architecture flow:\n",
                "1. **Input Processing**\n",
                "   - Takes variable groups:\n",
                "     - surface [B, V, H, W]\n",
                "     - atmospheric [B, V, L, H, W]  \n",
                "    where B - batch size, V - num variables, H - height, W - width, and L - pressure levels (if any)\n",
                "   - Processes each variable type separately\n",
                "   - Converts spatial data into patches [num_patches, patch_dim]\n",
                "\n",
                "2. **Embedding Process**\n",
                "   - Variable-specific embeddings\n",
                "   - Position encoding for spatial relationships\n",
                "   - Time encoding for temporal context\n",
                "   - Combines all embeddings into unified representation\n",
                "\n",
                "3. **Latent Generation**\n",
                "   - Uses Perceiver IO to create fixed-size latent representation\n",
                "   - Maintains relationships between variables\n",
                "   - Creates output of shape [B, num_latent_tokens, embed_dim]\n",
                "\n",
                "The encoder creates a compact, unified representation that captures the relationships between different variables while preserving spatial and temporal context."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "from einops import rearrange\n",
                "from bfm_model.perceiver_core.perceiver_io import PerceiverIO\n",
                "\n",
                "class SimpleBFMEncoder(nn.Module):\n",
                "    def __init__(\n",
                "        self,\n",
                "        surface_vars: tuple[str, ...],\n",
                "        atmos_vars: tuple[str, ...],\n",
                "        atmos_levels: tuple[int, ...],\n",
                "        patch_size: int = 4,\n",
                "        embed_dim: int = 128,\n",
                "        num_heads: int = 4,\n",
                "        num_latent_tokens: int = 8,\n",
                "        H: int = 152,\n",
                "        W: int = 320,\n",
                "    ):\n",
                "        super().__init__()\n",
                "        # some basic configuration variables\n",
                "        self.patch_size = patch_size\n",
                "        self.embed_dim = embed_dim\n",
                "        self.H = H\n",
                "        self.W = W\n",
                "        \n",
                "        # names of the used variables\n",
                "        self.surface_vars = surface_vars\n",
                "        self.atmos_vars = atmos_vars\n",
                "        self.atmos_levels = atmos_levels\n",
                "        \n",
                "        # we will use separate embeddings to encode each variable type\n",
                "        self.surface_embed = nn.Linear(len(surface_vars) * patch_size * patch_size, embed_dim)\n",
                "        self.atmos_embed = nn.Linear(len(atmos_vars) * patch_size * patch_size, embed_dim)\n",
                "        \n",
                "        # and universal position and time embeddings\n",
                "        self.pos_embed = nn.Linear(2, embed_dim)\n",
                "        self.time_embed = nn.Linear(1, embed_dim)\n",
                "        \n",
                "        # initialize Perceiver IO - the core of the encoder\n",
                "        num_patches = (H // patch_size) * (W // patch_size)\n",
                "        total_latents = num_latent_tokens\n",
                "        \n",
                "        self.perceiver = PerceiverIO(\n",
                "            num_layers=2,\n",
                "            dim=embed_dim,\n",
                "            queries_dim=embed_dim,\n",
                "            logits_dimension=None,\n",
                "            num_latent_tokens=total_latents,\n",
                "            latent_dimension=embed_dim,\n",
                "            cross_attention_heads=num_heads,\n",
                "            latent_attention_heads=num_heads,\n",
                "            cross_attention_head_dim=embed_dim // num_heads,\n",
                "            latent_attention_head_dim=embed_dim // num_heads,\n",
                "            sequence_dropout_prob=0.1,\n",
                "            num_input_axes=1\n",
                "        )\n",
                "        \n",
                "        # also the latent queries - they will be used to encode the data to the format we want\n",
                "        self.latents = nn.Parameter(torch.randn(total_latents, embed_dim))\n",
                "        \n",
                "        self.pre_perceiver_norm = nn.LayerNorm(embed_dim)\n",
                "        self.pos_drop = nn.Dropout(p=0.1)\n",
                "\n",
                "    def process_variables(self, variables: dict, embed_layer: nn.Module, name: str):\n",
                "        \"\"\"Process a group of variables\"\"\"\n",
                "        if not variables:\n",
                "            print(f\"\\n--- Processing {name}: No variables found ---\")\n",
                "            return None\n",
                "            \n",
                "        # stack variables [V, B, H, W]\n",
                "        x = torch.stack(list(variables.values()), dim=0)\n",
                "        print(f\"\\n--- Processing {name} ---\")\n",
                "        print(f\"Variables in group: {list(variables.keys())}\")\n",
                "        print(f\"Initial shape: {x.shape}\")\n",
                "        print(f\"  - num_variables (V): {x.shape[0]}\")\n",
                "        print(f\"  - batch_size (B): {x.shape[1]}\")\n",
                "        print(f\"  - height (H): {x.shape[2]}\")\n",
                "        print(f\"  - width (W): {x.shape[3]}\")\n",
                "        \n",
                "        # now we go over the first (and only) batch\n",
                "        x = x[:, 0]  # [V, H, W]\n",
                "        print(f\"\\nAfter selecting first batch:\")\n",
                "        print(f\"Shape: {x.shape}\")\n",
                "        print(f\"  - num_variables (V): {x.shape[0]}\")\n",
                "        print(f\"  - height (H): {x.shape[1]}\")\n",
                "        print(f\"  - width (W): {x.shape[2]}\")\n",
                "        \n",
                "        # reshape to patches - important step, as it allows us to handle the data in smaller chunks\n",
                "        x = rearrange(x, 'v (h p1) (w p2) -> (h w) (v p1 p2)', p1=self.patch_size, p2=self.patch_size)\n",
                "        num_patches_h = x.shape[0] // (self.W // self.patch_size)\n",
                "        print(f\"\\nAfter patch reshape:\")\n",
                "        print(f\"Shape: {x.shape}\")\n",
                "        print(f\"  - num_patches (h*w): {x.shape[0]} ({num_patches_h}×{x.shape[0]//num_patches_h})\")\n",
                "        print(f\"  - patch_dim (v*p1*p2): {x.shape[1]}\")\n",
                "        print(f\"  - patch_size: {self.patch_size}\")\n",
                "        \n",
                "        # embed - as simple as it gets (:\n",
                "        x = embed_layer(x)\n",
                "        print(f\"\\nAfter embedding:\")\n",
                "        print(f\"Shape: {x.shape}\")\n",
                "        print(f\"  - num_patches: {x.shape[0]}\")\n",
                "        print(f\"  - embed_dim: {x.shape[1]}\")\n",
                "        \n",
                "        return x\n",
                "\n",
                "    def forward(self, batch, lead_time):\n",
                "        device = next(self.parameters()).device\n",
                "        \n",
                "        # surface variables\n",
                "        surface_embed = self.process_variables(\n",
                "            batch.surface_variables, \n",
                "            self.surface_embed,\n",
                "            \"Surface Variables\"\n",
                "        )\n",
                "        \n",
                "        # atmospheric variables per level\n",
                "        atmos_embeds = []\n",
                "        for level_idx, level in enumerate(self.atmos_levels):\n",
                "            level_vars = {k: v[:, level_idx] for k, v in batch.atmospheric_variables.items()}\n",
                "            level_embed = self.process_variables(\n",
                "                level_vars,\n",
                "                self.atmos_embed,\n",
                "                f\"Atmospheric Level {level}\"\n",
                "            )\n",
                "            if level_embed is not None:\n",
                "                atmos_embeds.append(level_embed)\n",
                "        \n",
                "        # combine the embeddings\n",
                "        embeddings = []\n",
                "        if surface_embed is not None:\n",
                "            embeddings.append(surface_embed)\n",
                "        if atmos_embeds:\n",
                "            atmos_embed = torch.cat(atmos_embeds, dim=0)\n",
                "            embeddings.append(atmos_embed)\n",
                "        \n",
                "        # concat all embeddings\n",
                "        x = torch.cat(embeddings, dim=0)\n",
                "        x = x.unsqueeze(0)  # add batch dimension (not entirely necessary, as other steps here, but good practice is good practice)\n",
                "        print(f\"\\nCombined embeddings shape: {x.shape}\")\n",
                "        \n",
                "        # make the positions grid\n",
                "        pos = torch.stack(\n",
                "            torch.meshgrid(\n",
                "                batch.batch_metadata.latitudes[::self.patch_size],\n",
                "                batch.batch_metadata.longitudes[::self.patch_size],\n",
                "                indexing=\"ij\"\n",
                "            ),\n",
                "            dim=-1\n",
                "        )\n",
                "        \n",
                "        # we flatten the positions to a single axis - less dimensions, less headache\n",
                "        pos = pos.reshape(-1, 2)  # shape: [num_patches, 2]\n",
                "        \n",
                "        # normalize positions to [-1, 1], for numerical stability\n",
                "        pos = 2 * (pos - pos.min(dim=0)[0]) / (pos.max(dim=0)[0] - pos.min(dim=0)[0]) - 1\n",
                "        \n",
                "        # again, good practice\n",
                "        pos = pos.unsqueeze(0)  # shape: [1, num_patches, 2]\n",
                "        \n",
                "        # embedding the positions\n",
                "        pos_embed = self.pos_embed(pos)  # shape: [1, num_patches, embed_dim]\n",
                "        \n",
                "        # expand the position embeddings for all variable groups\n",
                "        num_var_groups = x.shape[1] // pos_embed.shape[1]\n",
                "        pos_embed = pos_embed.repeat_interleave(num_var_groups, dim=1)\n",
                "        \n",
                "        # add the position encodings to the data\n",
                "        x = x + pos_embed\n",
                "        \n",
                "        # aand the time encoding\n",
                "        time = torch.tensor([[lead_time.total_seconds() / 3600]], device=device)\n",
                "        time_embed = self.time_embed(time)\n",
                "        x = x + time_embed.unsqueeze(1)\n",
                "        \n",
                "        # lastly, normalize and apply Perceiver IO to get our final embeddings\n",
                "        x = self.pre_perceiver_norm(x)\n",
                "        latents = self.latents.unsqueeze(0)\n",
                "        x = self.perceiver(x, queries=latents)\n",
                "        \n",
                "        return self.pos_drop(x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_sample_batch(batch_size=1, H=152, W=320):\n",
                "    \"\"\"\n",
                "    Create a sample batch with the same structure as the real data but simplified.\n",
                "    \n",
                "    Returns:\n",
                "        SimpleBatch with:\n",
                "        - surface_variables: [B, H, W]\n",
                "        - atmospheric_variables: [B, L, H, W]\n",
                "        where:\n",
                "        B = batch_size (default 1)\n",
                "        L = pressure levels (2)\n",
                "        H = height (152)\n",
                "        W = width (320)\n",
                "    \"\"\"\n",
                "    # make metadata\n",
                "    metadata = SimpleBatchMetadata(\n",
                "        latitudes=torch.linspace(72.0, 34.0, H),  # fun fact: I have no idea where this area would be\n",
                "        longitudes=torch.linspace(0.0, 359.75, W),\n",
                "        pressure_levels=(1000, 50),\n",
                "        timestamp=(\n",
                "            datetime(2024, 1, 1, 0, 0),\n",
                "            datetime(2024, 1, 2, 0, 0)\n",
                "        )\n",
                "    )\n",
                "    \n",
                "    # surface variables (t2m, msl)\n",
                "    surface_vars = {\n",
                "        \"t2m\": torch.randn(batch_size, H, W),  # temperature\n",
                "        \"msl\": torch.randn(batch_size, H, W)   # mean sea level pressure\n",
                "    }\n",
                "    \n",
                "    # and some atmospheric variables (z, u, v)\n",
                "    atmos_vars = {\n",
                "        \"z\": torch.randn(batch_size, len(metadata.pressure_levels), H, W),\n",
                "        \"u\": torch.randn(batch_size, len(metadata.pressure_levels), H, W),\n",
                "        \"v\": torch.randn(batch_size, len(metadata.pressure_levels), H, W)\n",
                "    }\n",
                "    \n",
                "    return SimpleBatch(\n",
                "        surface_variables=surface_vars,\n",
                "        atmospheric_variables=atmos_vars,\n",
                "        batch_metadata=metadata\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Processing sample batch...\n",
                        "\n",
                        "--- Processing Surface Variables ---\n",
                        "Variables in group: ['t2m', 'msl']\n",
                        "Initial shape: torch.Size([2, 1, 152, 320])\n",
                        "  - num_variables (V): 2\n",
                        "  - batch_size (B): 1\n",
                        "  - height (H): 152\n",
                        "  - width (W): 320\n",
                        "\n",
                        "After selecting first batch:\n",
                        "Shape: torch.Size([2, 152, 320])\n",
                        "  - num_variables (V): 2\n",
                        "  - height (H): 152\n",
                        "  - width (W): 320\n",
                        "\n",
                        "After patch reshape:\n",
                        "Shape: torch.Size([3040, 32])\n",
                        "  - num_patches (h*w): 3040 (38×80)\n",
                        "  - patch_dim (v*p1*p2): 32\n",
                        "  - patch_size: 4\n",
                        "\n",
                        "After embedding:\n",
                        "Shape: torch.Size([3040, 128])\n",
                        "  - num_patches: 3040\n",
                        "  - embed_dim: 128\n",
                        "\n",
                        "--- Processing Atmospheric Level 1000 ---\n",
                        "Variables in group: ['z', 'u', 'v']\n",
                        "Initial shape: torch.Size([3, 1, 152, 320])\n",
                        "  - num_variables (V): 3\n",
                        "  - batch_size (B): 1\n",
                        "  - height (H): 152\n",
                        "  - width (W): 320\n",
                        "\n",
                        "After selecting first batch:\n",
                        "Shape: torch.Size([3, 152, 320])\n",
                        "  - num_variables (V): 3\n",
                        "  - height (H): 152\n",
                        "  - width (W): 320\n",
                        "\n",
                        "After patch reshape:\n",
                        "Shape: torch.Size([3040, 48])\n",
                        "  - num_patches (h*w): 3040 (38×80)\n",
                        "  - patch_dim (v*p1*p2): 48\n",
                        "  - patch_size: 4\n",
                        "\n",
                        "After embedding:\n",
                        "Shape: torch.Size([3040, 128])\n",
                        "  - num_patches: 3040\n",
                        "  - embed_dim: 128\n",
                        "\n",
                        "--- Processing Atmospheric Level 50 ---\n",
                        "Variables in group: ['z', 'u', 'v']\n",
                        "Initial shape: torch.Size([3, 1, 152, 320])\n",
                        "  - num_variables (V): 3\n",
                        "  - batch_size (B): 1\n",
                        "  - height (H): 152\n",
                        "  - width (W): 320\n",
                        "\n",
                        "After selecting first batch:\n",
                        "Shape: torch.Size([3, 152, 320])\n",
                        "  - num_variables (V): 3\n",
                        "  - height (H): 152\n",
                        "  - width (W): 320\n",
                        "\n",
                        "After patch reshape:\n",
                        "Shape: torch.Size([3040, 48])\n",
                        "  - num_patches (h*w): 3040 (38×80)\n",
                        "  - patch_dim (v*p1*p2): 48\n",
                        "  - patch_size: 4\n",
                        "\n",
                        "After embedding:\n",
                        "Shape: torch.Size([3040, 128])\n",
                        "  - num_patches: 3040\n",
                        "  - embed_dim: 128\n",
                        "\n",
                        "Combined embeddings shape: torch.Size([1, 9120, 128])\n",
                        "\n",
                        "Success!\n",
                        "Input shapes:\n",
                        "Surface variables: torch.Size([1, 152, 320])\n",
                        "Atmospheric variables: torch.Size([1, 2, 152, 320])\n",
                        "Output shape: torch.Size([1, 8, 128])\n",
                        "Expected shape: [batch_size=1, num_latent_tokens=8, embed_dim=128]\n"
                    ]
                }
            ],
            "source": [
                "def main():\n",
                "    encoder = SimpleBFMEncoder(\n",
                "        surface_vars=(\"t2m\", \"msl\"),\n",
                "        atmos_vars=(\"z\", \"u\", \"v\"),\n",
                "        atmos_levels=(1000, 50),\n",
                "        patch_size=4,\n",
                "        embed_dim=128,\n",
                "        num_heads=4,\n",
                "        num_latent_tokens=8,\n",
                "        H=152,\n",
                "        W=320\n",
                "    )\n",
                "    batch = create_sample_batch()\n",
                "    print(\"\\nProcessing sample batch...\")\n",
                "    lead_time = timedelta(hours=24)  # that means we want to predict 24 hours ahead\n",
                "    \n",
                "    try:\n",
                "        output = encoder(batch, lead_time)\n",
                "        print(\"\\nSuccess!\")\n",
                "        print(f\"Input shapes:\")\n",
                "        print(f\"Surface variables: {next(iter(batch.surface_variables.values())).shape}\")\n",
                "        print(f\"Atmospheric variables: {next(iter(batch.atmospheric_variables.values())).shape}\")\n",
                "        print(f\"Output shape: {output.shape}\")\n",
                "        print(f\"Expected shape: [batch_size=1, num_latent_tokens=8, embed_dim=128]\")\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"\\nError processing batch\")\n",
                "        print(f\"Error message: {str(e)}\")\n",
                "        raise e\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You saw what happened? Let's summarize what *happened* here:  \n",
                "ur data consists of two main variable types:\n",
                "\n",
                "1. **Surface Variables** [B, H, W]  \n",
                "   - `t2m`: 2-meter temperature  \n",
                "   - `msl`: mean sea level pressure  \n",
                "\n",
                "2. **Atmospheric Variables** [B, L, H, W]\n",
                "   - `z`: geopotential\n",
                "   - `u`: u-wind component\n",
                "   - `v`: v-wind component\n",
                "   - Measured at pressure levels: 1000hPa and 50hPa\n",
                "\n",
                "Where:\n",
                "- `B`: batch size (1)\n",
                "- `L`: pressure levels (2)\n",
                "- `H`: height (152)\n",
                "- `W`: width (320)\n",
                "- Spatial coverage: 72N to 34N, 0 to 359.75E\n",
                "- Patch size: 4 x 4\n",
                "\n",
                "Each variable is processed into patches, embedded, and then combined before being passed through a Perceiver IO architecture. That would gives us our compacted an enriched with information representation of the data.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <span style=\"color: orange;\">BFM Decoder Analysis</span>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The decoder is responsible for:\n",
                "1. Converting latent representations back to physical space;\n",
                "2. Reconstructing different variable types from the encoded embeddings;\n",
                "3. Maintaining spatial relationships through position encodings;\n",
                "4. Preserving temporal context in reconstructions.\n",
                "\n",
                "Key design patterns:\n",
                "- Token projections for each variable type;\n",
                "- Inverse position encoding to maintain spatial structure;\n",
                "- Query-based decoding through Perceiver IO;\n",
                "- Variable-specific output heads;\n",
                "- Structured output format matching input variables.\n",
                "\n",
                "Architecture flow:\n",
                "1. **Input Processing**\n",
                "   - Takes encoded latent representation [B, L, D];\n",
                "   - Creates variable-specific queries;\n",
                "   - Applies position and time encodings to queries.\n",
                "\n",
                "2. **Decoding Process**\n",
                "   - Uses Perceiver IO for cross-attention between latents and queries;\n",
                "   - Maintains separate processing paths for each variable type;\n",
                "   - Preserves spatial dimensions through token projections.\n",
                "\n",
                "3. **Output Generation**\n",
                "   - Reconstructs surface variables [B, V, H, W];\n",
                "   - Reconstructs atmospheric variables [B, V, L, H, W];\n",
                "   - Maintains variable relationships and physical constraints.\n",
                "\n",
                "The decoder mirrors the encoder's structure but in reverse, ensuring that the output matches the input format while preserving learned representations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleBFMDecoder(nn.Module):\n",
                "    def __init__(\n",
                "        self,\n",
                "        surface_vars: tuple[str, ...],\n",
                "        atmos_vars: tuple[str, ...],\n",
                "        atmos_levels: tuple[int, ...],\n",
                "        patch_size: int = 4,\n",
                "        embed_dim: int = 128,\n",
                "        num_heads: int = 4,\n",
                "        H: int = 152,\n",
                "        W: int = 320,\n",
                "    ):\n",
                "        super().__init__()\n",
                "         # again, some basic configuration variables\n",
                "        self.embed_dim = embed_dim\n",
                "        self.H = H\n",
                "        self.W = W\n",
                "        self.patch_size = patch_size\n",
                "\n",
                "        # names of the used variables\n",
                "        self.surface_vars = surface_vars\n",
                "        self.atmos_vars = atmos_vars\n",
                "        self.atmos_levels = atmos_levels\n",
                "        \n",
                "        # calculate the number of patches in the grid\n",
                "        self.num_patches_h = H // patch_size\n",
                "        self.num_patches_w = W // patch_size\n",
                "        self.num_patches = self.num_patches_h * self.num_patches_w\n",
                "        \n",
                "        # position and time embeddings, just like in the encoder\n",
                "        self.pos_embed = nn.Linear(2, embed_dim)\n",
                "        self.time_embed = nn.Linear(1, embed_dim)\n",
                "        \n",
                "        # output projections for each variable type\n",
                "        self.surface_proj = nn.Linear(embed_dim, len(surface_vars))\n",
                "        self.atmos_proj = nn.Linear(embed_dim, len(atmos_vars))\n",
                "        \n",
                "        # initialize Perceiver IO\n",
                "        total_queries = self.num_patches  # one query per patch - we want to predict each patch, and we have one query per patch\n",
                "        \n",
                "        self.perceiver = PerceiverIO(\n",
                "            num_layers=2,\n",
                "            dim=embed_dim,\n",
                "            queries_dim=embed_dim,\n",
                "            logits_dimension=None,\n",
                "            num_latent_tokens=total_queries,\n",
                "            latent_dimension=embed_dim,\n",
                "            cross_attention_heads=num_heads,\n",
                "            latent_attention_heads=num_heads,\n",
                "            cross_attention_head_dim=embed_dim // num_heads,\n",
                "            latent_attention_head_dim=embed_dim // num_heads,\n",
                "            sequence_dropout_prob=0.1,\n",
                "            num_input_axes=1\n",
                "        )\n",
                "        \n",
                "        self.pos_drop = nn.Dropout(p=0.1)\n",
                "        self.apply(self._init_weights)\n",
                "    \n",
                "    def _init_weights(self, m):\n",
                "        # this method is also used in the original decoder - it basically just initializes the weights with a truncated normal distribution\n",
                "        if isinstance(m, nn.Linear):\n",
                "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
                "            if m.bias is not None:\n",
                "                nn.init.constant_(m.bias, 0)\n",
                "        elif isinstance(m, nn.LayerNorm):\n",
                "            nn.init.constant_(m.bias, 0)\n",
                "            nn.init.constant_(m.weight, 1.0)\n",
                "\n",
                "    def forward(self, x, batch, lead_time):\n",
                "        B = x.shape[0] \n",
                "        device = x.device\n",
                "        \n",
                "        print(f\"\\n--- Decoder Forward Pass ---\")\n",
                "        print(f\"Input shape: {x.shape}\")\n",
                "        print(f\"Grid dimensions (H×W): {self.H}×{self.W}\")\n",
                "        print(f\"Patch dimensions: {self.num_patches_h}×{self.num_patches_w}\")\n",
                "        \n",
                "        # make the positions grid, as in the encoder\n",
                "        pos = torch.stack(\n",
                "            torch.meshgrid(\n",
                "                torch.linspace(-1, 1, self.num_patches_h, device=device),\n",
                "                torch.linspace(-1, 1, self.num_patches_w, device=device),\n",
                "                indexing=\"ij\"\n",
                "            ),\n",
                "            dim=-1\n",
                "        ).reshape(-1, 2)\n",
                "        \n",
                "        # make queries with position encoding\n",
                "        queries = self.pos_embed(pos)  # shape: [num_patches, embed_dim]\n",
                "        queries = queries.unsqueeze(0).expand(B, -1, -1)  # shape: [B, num_patches, embed_dim]\n",
                "        \n",
                "        # add the time encoding\n",
                "        time = torch.tensor([[lead_time.total_seconds() / 3600]], device=device)\n",
                "        time_embed = self.time_embed(time)\n",
                "        queries = queries + time_embed.unsqueeze(1)\n",
                "        \n",
                "        # apply Perceiver IO\n",
                "        decoded = self.perceiver(x, queries=queries)\n",
                "        print(f\"Decoded patches shape: {decoded.shape}\")\n",
                "        \n",
                "        # process surface variables\n",
                "        surface_out = self.surface_proj(decoded)  # shape: [B, num_patches, num_surface_vars]\n",
                "        surface_out = surface_out.permute(0, 2, 1)  # shape: [B, num_surface_vars, num_patches]\n",
                "        surface_out = surface_out.view(B, len(self.surface_vars), \n",
                "                                     self.num_patches_h, self.num_patches_w)\n",
                "        surface_out = F.interpolate(surface_out, size=(self.H, self.W), \n",
                "                                  mode='bilinear', align_corners=False)\n",
                "        \n",
                "        # process atmospheric variables\n",
                "        atmos_out = self.atmos_proj(decoded)  # shape: [B, num_patches, num_atmos_vars]\n",
                "        atmos_out = atmos_out.permute(0, 2, 1)  # shape: [B, num_atmos_vars, num_patches]\n",
                "        atmos_out = atmos_out.view(B, len(self.atmos_vars), \n",
                "                                  self.num_patches_h, self.num_patches_w)\n",
                "        atmos_out = F.interpolate(atmos_out, size=(self.H, self.W), \n",
                "                                 mode='bilinear', align_corners=False)\n",
                "        \n",
                "        # expand atmospheric variables for each level\n",
                "        atmos_out = atmos_out.unsqueeze(2).expand(-1, -1, len(self.atmos_levels), -1, -1)\n",
                "        \n",
                "        print(f\"\\nOutput shapes:\")\n",
                "        print(f\"Surface variables: {surface_out.shape}\")\n",
                "        print(f\"Atmospheric variables: {atmos_out.shape}\")\n",
                "        \n",
                "        return {\n",
                "            \"surface_variables\": {\n",
                "                name: surface_out[:, i] \n",
                "                for i, name in enumerate(self.surface_vars)\n",
                "            },\n",
                "            \"atmospheric_variables\": {\n",
                "                name: atmos_out[:, i] \n",
                "                for i, name in enumerate(self.atmos_vars)\n",
                "            }\n",
                "        }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Processing Surface Variables ---\n",
                        "Variables in group: ['t2m', 'msl']\n",
                        "Initial shape: torch.Size([2, 1, 152, 320])\n",
                        "  - num_variables (V): 2\n",
                        "  - batch_size (B): 1\n",
                        "  - height (H): 152\n",
                        "  - width (W): 320\n",
                        "\n",
                        "After selecting first batch:\n",
                        "Shape: torch.Size([2, 152, 320])\n",
                        "  - num_variables (V): 2\n",
                        "  - height (H): 152\n",
                        "  - width (W): 320\n",
                        "\n",
                        "After patch reshape:\n",
                        "Shape: torch.Size([3040, 32])\n",
                        "  - num_patches (h*w): 3040 (38×80)\n",
                        "  - patch_dim (v*p1*p2): 32\n",
                        "  - patch_size: 4\n",
                        "\n",
                        "After embedding:\n",
                        "Shape: torch.Size([3040, 128])\n",
                        "  - num_patches: 3040\n",
                        "  - embed_dim: 128\n",
                        "\n",
                        "--- Processing Atmospheric Level 1000 ---\n",
                        "Variables in group: ['z', 'u', 'v']\n",
                        "Initial shape: torch.Size([3, 1, 152, 320])\n",
                        "  - num_variables (V): 3\n",
                        "  - batch_size (B): 1\n",
                        "  - height (H): 152\n",
                        "  - width (W): 320\n",
                        "\n",
                        "After selecting first batch:\n",
                        "Shape: torch.Size([3, 152, 320])\n",
                        "  - num_variables (V): 3\n",
                        "  - height (H): 152\n",
                        "  - width (W): 320\n",
                        "\n",
                        "After patch reshape:\n",
                        "Shape: torch.Size([3040, 48])\n",
                        "  - num_patches (h*w): 3040 (38×80)\n",
                        "  - patch_dim (v*p1*p2): 48\n",
                        "  - patch_size: 4\n",
                        "\n",
                        "After embedding:\n",
                        "Shape: torch.Size([3040, 128])\n",
                        "  - num_patches: 3040\n",
                        "  - embed_dim: 128\n",
                        "\n",
                        "--- Processing Atmospheric Level 50 ---\n",
                        "Variables in group: ['z', 'u', 'v']\n",
                        "Initial shape: torch.Size([3, 1, 152, 320])\n",
                        "  - num_variables (V): 3\n",
                        "  - batch_size (B): 1\n",
                        "  - height (H): 152\n",
                        "  - width (W): 320\n",
                        "\n",
                        "After selecting first batch:\n",
                        "Shape: torch.Size([3, 152, 320])\n",
                        "  - num_variables (V): 3\n",
                        "  - height (H): 152\n",
                        "  - width (W): 320\n",
                        "\n",
                        "After patch reshape:\n",
                        "Shape: torch.Size([3040, 48])\n",
                        "  - num_patches (h*w): 3040 (38×80)\n",
                        "  - patch_dim (v*p1*p2): 48\n",
                        "  - patch_size: 4\n",
                        "\n",
                        "After embedding:\n",
                        "Shape: torch.Size([3040, 128])\n",
                        "  - num_patches: 3040\n",
                        "  - embed_dim: 128\n",
                        "\n",
                        "Combined embeddings shape: torch.Size([1, 9120, 128])\n",
                        "\n",
                        "Encoded shape: torch.Size([1, 8, 128])\n",
                        "\n",
                        "--- Decoder Forward Pass ---\n",
                        "Input shape: torch.Size([1, 8, 128])\n",
                        "Grid dimensions (H×W): 152×320\n",
                        "Patch dimensions: 38×80\n",
                        "Decoded patches shape: torch.Size([1, 3040, 128])\n",
                        "\n",
                        "Output shapes:\n",
                        "Surface variables: torch.Size([1, 2, 152, 320])\n",
                        "Atmospheric variables: torch.Size([1, 3, 2, 152, 320])\n",
                        "\n",
                        "Decoded outputs:\n",
                        "surface_variables:\n",
                        "  - t2m: torch.Size([1, 152, 320])\n",
                        "  - msl: torch.Size([1, 152, 320])\n",
                        "atmospheric_variables:\n",
                        "  - z: torch.Size([1, 2, 152, 320])\n",
                        "  - u: torch.Size([1, 2, 152, 320])\n",
                        "  - v: torch.Size([1, 2, 152, 320])\n"
                    ]
                }
            ],
            "source": [
                "def test_decoder():\n",
                "    # Create encoder and decoder\n",
                "    encoder = SimpleBFMEncoder(\n",
                "        surface_vars=(\"t2m\", \"msl\"),\n",
                "        atmos_vars=(\"z\", \"u\", \"v\"),\n",
                "        atmos_levels=(1000, 50),\n",
                "        patch_size=4,\n",
                "        embed_dim=128,\n",
                "        num_heads=4,\n",
                "        num_latent_tokens=8,\n",
                "        H=152,\n",
                "        W=320\n",
                "    )\n",
                "    \n",
                "    decoder = SimpleBFMDecoder(\n",
                "        surface_vars=(\"t2m\", \"msl\"),\n",
                "        atmos_vars=(\"z\", \"u\", \"v\"),\n",
                "        atmos_levels=(1000, 50),\n",
                "        patch_size=4,\n",
                "        embed_dim=128,\n",
                "        num_heads=4,\n",
                "        H=152,\n",
                "        W=320\n",
                "    )\n",
                "    \n",
                "    # Create sample batch\n",
                "    batch = create_sample_batch()\n",
                "    lead_time = timedelta(hours=24)\n",
                "    \n",
                "    # Process through encoder and decoder\n",
                "    try:\n",
                "        encoded = encoder(batch, lead_time)\n",
                "        print(\"\\nEncoded shape:\", encoded.shape)\n",
                "        \n",
                "        decoded = decoder(encoded, batch, lead_time)\n",
                "        print(\"\\nDecoded outputs:\")\n",
                "        for var_type, vars in decoded.items():\n",
                "            print(f\"{var_type}:\")\n",
                "            for var_name, var_data in vars.items():\n",
                "                print(f\"  - {var_name}: {var_data.shape}\")\n",
                "                \n",
                "    except Exception as e:\n",
                "        print(f\"\\nError in processing:\")\n",
                "        print(f\"Error message: {str(e)}\")\n",
                "        raise e\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    test_decoder()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <span style=\"color: orange;\">Biodiversity Foundation Model</span>\n",
                "Below, we will create a simplified version of the BFM, which will be used to process our data and demonstrate how its bigger brother works."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Processing Surface Variables ---\n",
                        "Variables in group: ['t2m', 'msl']\n",
                        "Initial shape: torch.Size([2, 1, 152, 320])\n",
                        "  - num_variables (V): 2\n",
                        "  - batch_size (B): 1\n",
                        "  - height (H): 152\n",
                        "  - width (W): 320\n",
                        "\n",
                        "After selecting first batch:\n",
                        "Shape: torch.Size([2, 152, 320])\n",
                        "  - num_variables (V): 2\n",
                        "  - height (H): 152\n",
                        "  - width (W): 320\n",
                        "\n",
                        "After patch reshape:\n",
                        "Shape: torch.Size([3040, 32])\n",
                        "  - num_patches (h*w): 3040 (38×80)\n",
                        "  - patch_dim (v*p1*p2): 32\n",
                        "  - patch_size: 4\n",
                        "\n",
                        "After embedding:\n",
                        "Shape: torch.Size([3040, 128])\n",
                        "  - num_patches: 3040\n",
                        "  - embed_dim: 128\n",
                        "\n",
                        "--- Processing Atmospheric Level 1000 ---\n",
                        "Variables in group: ['z', 'u', 'v']\n",
                        "Initial shape: torch.Size([3, 1, 152, 320])\n",
                        "  - num_variables (V): 3\n",
                        "  - batch_size (B): 1\n",
                        "  - height (H): 152\n",
                        "  - width (W): 320\n",
                        "\n",
                        "After selecting first batch:\n",
                        "Shape: torch.Size([3, 152, 320])\n",
                        "  - num_variables (V): 3\n",
                        "  - height (H): 152\n",
                        "  - width (W): 320\n",
                        "\n",
                        "After patch reshape:\n",
                        "Shape: torch.Size([3040, 48])\n",
                        "  - num_patches (h*w): 3040 (38×80)\n",
                        "  - patch_dim (v*p1*p2): 48\n",
                        "  - patch_size: 4\n",
                        "\n",
                        "After embedding:\n",
                        "Shape: torch.Size([3040, 128])\n",
                        "  - num_patches: 3040\n",
                        "  - embed_dim: 128\n",
                        "\n",
                        "--- Processing Atmospheric Level 50 ---\n",
                        "Variables in group: ['z', 'u', 'v']\n",
                        "Initial shape: torch.Size([3, 1, 152, 320])\n",
                        "  - num_variables (V): 3\n",
                        "  - batch_size (B): 1\n",
                        "  - height (H): 152\n",
                        "  - width (W): 320\n",
                        "\n",
                        "After selecting first batch:\n",
                        "Shape: torch.Size([3, 152, 320])\n",
                        "  - num_variables (V): 3\n",
                        "  - height (H): 152\n",
                        "  - width (W): 320\n",
                        "\n",
                        "After patch reshape:\n",
                        "Shape: torch.Size([3040, 48])\n",
                        "  - num_patches (h*w): 3040 (38×80)\n",
                        "  - patch_dim (v*p1*p2): 48\n",
                        "  - patch_size: 4\n",
                        "\n",
                        "After embedding:\n",
                        "Shape: torch.Size([3040, 128])\n",
                        "  - num_patches: 3040\n",
                        "  - embed_dim: 128\n",
                        "\n",
                        "Combined embeddings shape: torch.Size([1, 9120, 128])\n",
                        "\n",
                        "--- Decoder Forward Pass ---\n",
                        "Input shape: torch.Size([1, 8, 128])\n",
                        "Grid dimensions (H×W): 152×320\n",
                        "Patch dimensions: 38×80\n",
                        "Decoded patches shape: torch.Size([1, 3040, 128])\n",
                        "\n",
                        "Output shapes:\n",
                        "Surface variables: torch.Size([1, 2, 152, 320])\n",
                        "Atmospheric variables: torch.Size([1, 3, 2, 152, 320])\n",
                        "\n",
                        "Model outputs:\n",
                        "\n",
                        "surface_variables:\n",
                        "  - t2m: torch.Size([1, 152, 320])\n",
                        "  - msl: torch.Size([1, 152, 320])\n",
                        "\n",
                        "atmospheric_variables:\n",
                        "  - z: torch.Size([1, 2, 152, 320])\n",
                        "  - u: torch.Size([1, 2, 152, 320])\n",
                        "  - v: torch.Size([1, 2, 152, 320])\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "from datetime import timedelta\n",
                "\n",
                "class SimpleBFM(nn.Module):\n",
                "    def __init__(\n",
                "        self,\n",
                "        surface_vars: tuple[str, ...],\n",
                "        atmos_vars: tuple[str, ...],\n",
                "        atmos_levels: tuple[int, ...],\n",
                "        H: int = 152,\n",
                "        W: int = 320,\n",
                "        embed_dim: int = 128,\n",
                "        num_latent_tokens: int = 8,\n",
                "        patch_size: int = 4,\n",
                "        num_heads: int = 4,\n",
                "    ):\n",
                "        super().__init__()\n",
                "        \n",
                "        # initialize encoder\n",
                "        self.encoder = SimpleBFMEncoder(\n",
                "            surface_vars=surface_vars,\n",
                "            atmos_vars=atmos_vars,\n",
                "            atmos_levels=atmos_levels,\n",
                "            patch_size=patch_size,\n",
                "            embed_dim=embed_dim,\n",
                "            num_heads=num_heads,\n",
                "            num_latent_tokens=num_latent_tokens,\n",
                "            H=H,\n",
                "            W=W,\n",
                "        )\n",
                "        \n",
                "        # simplified backbone (identity function)\n",
                "        # Note: In the full BFM, this would be either MViT or Swin Transformer\n",
                "        # both maintain input dimensions (i.e., shapes and sizes), so for demonstration we use identity\n",
                "        self.backbone = nn.Identity()\n",
                "        \n",
                "        # initialize decoder\n",
                "        self.decoder = SimpleBFMDecoder(\n",
                "            surface_vars=surface_vars,\n",
                "            atmos_vars=atmos_vars,\n",
                "            atmos_levels=atmos_levels,\n",
                "            patch_size=patch_size,\n",
                "            embed_dim=embed_dim,\n",
                "            num_heads=num_heads,\n",
                "            H=H,\n",
                "            W=W,\n",
                "        )\n",
                "\n",
                "    def forward(self, batch, lead_time):\n",
                "        # encode\n",
                "        encoded = self.encoder(batch, lead_time)\n",
                "        \n",
                "        # process through backbone\n",
                "        # in full BFM, this would do temporal/spatial processing\n",
                "        # both MViT and Swin maintain shape: [B, num_tokens, embed_dim]\n",
                "        processed = self.backbone(encoded)\n",
                "        \n",
                "        # decode\n",
                "        output = self.decoder(processed, batch, lead_time)\n",
                "        \n",
                "        return output\n",
                "\n",
                "\n",
                "def test_simple_bfm():\n",
                "    model = SimpleBFM(\n",
                "        surface_vars=(\"t2m\", \"msl\"),\n",
                "        atmos_vars=(\"z\", \"u\", \"v\"),\n",
                "        atmos_levels=(1000, 50),\n",
                "    )\n",
                "    \n",
                "    # create sample batch\n",
                "    batch = create_sample_batch()\n",
                "    lead_time = timedelta(hours=24)\n",
                "    \n",
                "    # process batch\n",
                "    try:\n",
                "        output = model(batch, lead_time)\n",
                "        \n",
                "        print(\"\\nModel outputs:\")\n",
                "        for var_type, vars in output.items():\n",
                "            print(f\"\\n{var_type}:\")\n",
                "            for var_name, var_data in vars.items():\n",
                "                print(f\"  - {var_name}: {var_data.shape}\")\n",
                "                \n",
                "    except Exception as e:\n",
                "        print(f\"\\nError in processing:\")\n",
                "        print(f\"Error message: {str(e)}\")\n",
                "        raise e\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    test_simple_bfm()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <span style=\"color: orange;\">Adapting BFM Architecture for Air Quality Prediction</span>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this section, we'll explore how the BFM architecture can be adapted for different types of environmental data. We'll use air quality prediction as an example, demonstrating the flexibility of our encoder-decoder architecture.\n",
                "\n",
                "Key differences from climate prediction, in the current context:\n",
                "1. **Data structure**\n",
                "   - Climate: Spatial grids (H x W) with multiple variable types\n",
                "   - Air Quality: Time series data with sensor readings and ground truth\n",
                "\n",
                "2. **Variable types**\n",
                "   - Climate: Surface, atmospheric, species, land variables\n",
                "   - Air Quality: Sensor readings (PT08.S*), ground truth measurements (*GT), physical parameters\n",
                "\n",
                "3. **Temporal aspect**\n",
                "   - Climate: Predicting spatial patterns at future timesteps\n",
                "   - Air Quality: Predicting sensor values for the next hour based on historical readings\n",
                "\n",
                "We'll create a simplified version of the Air Quality Foundation Model (AQFM) to demonstrate how the core architecture can be adapted while maintaining the same conceptual structure of encoder → backbone → decoder."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <span style=\"color: orange;\">AQFM Encoder</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleAQEncoder(nn.Module):\n",
                "    def __init__(\n",
                "        self, \n",
                "        feature_names, \n",
                "        embed_dim, \n",
                "        num_latent_tokens, \n",
                "        max_history_size,\n",
                "        num_heads=8,\n",
                "        head_dim=64,\n",
                "        depth=2,\n",
                "        drop_rate=0.1\n",
                "    ):\n",
                "        super().__init__()\n",
                "        self.feature_names = feature_names\n",
                "        self.embed_dim = embed_dim\n",
                "        self.num_latent_tokens = num_latent_tokens\n",
                "        self.max_history_size = max_history_size\n",
                "        \n",
                "        # create embeddings for each feature group\n",
                "        self.sensor_embed = nn.Linear(len(feature_names['sensor']), embed_dim)\n",
                "        self.ground_truth_embed = nn.Linear(len(feature_names['ground_truth']), embed_dim)\n",
                "        self.physical_embed = nn.Linear(len(feature_names['physical']), embed_dim)\n",
                "        \n",
                "        # initialize latent queries\n",
                "        self.latents = nn.Parameter(torch.randn(num_latent_tokens, embed_dim))\n",
                "        \n",
                "        # add perceiver io\n",
                "        self.perceiver = PerceiverIO(\n",
                "            num_layers=depth,\n",
                "            dim=embed_dim,\n",
                "            queries_dim=embed_dim,\n",
                "            logits_dimension=None,\n",
                "            num_latent_tokens=num_latent_tokens,\n",
                "            latent_dimension=embed_dim,\n",
                "            cross_attention_heads=num_heads,\n",
                "            latent_attention_heads=num_heads,\n",
                "            cross_attention_head_dim=head_dim,\n",
                "            latent_attention_head_dim=head_dim,\n",
                "            sequence_dropout_prob=drop_rate,\n",
                "            num_fourier_bands=32,\n",
                "            max_frequency=max_history_size,\n",
                "            num_input_axes=1,\n",
                "            position_encoding_type=\"fourier\"\n",
                "        )\n",
                "        \n",
                "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
                "        \n",
                "    def forward(self, batch, lead_time):\n",
                "        # process sensor variables\n",
                "        sensor_vars = torch.stack([batch.sensor_vars[name] for name in self.feature_names['sensor']], dim=-1)\n",
                "        sensor_embed = self.sensor_embed(sensor_vars)  # [b, t, embed_dim]\n",
                "        \n",
                "        # process ground truth variables\n",
                "        ground_truth_vars = torch.stack([batch.ground_truth_vars[name] for name in self.feature_names['ground_truth']], dim=-1)\n",
                "        ground_truth_embed = self.ground_truth_embed(ground_truth_vars)  # [b, t, embed_dim]\n",
                "        \n",
                "        # process physical variables\n",
                "        physical_vars = torch.stack([batch.physical_vars[name] for name in self.feature_names['physical']], dim=-1)\n",
                "        physical_embed = self.physical_embed(physical_vars)  # [b, t, embed_dim]\n",
                "        \n",
                "        # combine embeddings\n",
                "        combined_embed = sensor_embed + ground_truth_embed + physical_embed  # [b, t, embed_dim]\n",
                "        \n",
                "        # get batch size\n",
                "        B = combined_embed.size(0) if len(combined_embed.size()) > 2 else 1\n",
                "        \n",
                "        # prepare latent queries\n",
                "        latents = self.latents.unsqueeze(0).expand(B, -1, -1)  # [b, num_latent_tokens, embed_dim]\n",
                "        \n",
                "        # process through perceiver io\n",
                "        encoded = self.perceiver(combined_embed, queries=latents)\n",
                "        encoded = self.pos_drop(encoded)\n",
                "        \n",
                "        return encoded"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The transition from BFM's spatial climate data to AQFM's temporal air quality data required several fundamental changes in the encoder architecture. Let's explore these adaptations in detail:\n",
                "\n",
                "### 1. Data Structure and Variable Handling\n",
                "In the BFM, we dealt with spatial grids where each variable was represented as a 2D map (H x W). This required patch-based processing, where the encoder would divide these spatial maps into smaller patches (patch_size x patch_size) before embedding. The AQFM, however, handles time series data where each variable is a 1D sequence of measurements over time. This fundamental difference eliminated the need for patch-based processing, simplifying our embedding approach.\n",
                "\n",
                "Instead of the BFM's separate handling of surface and atmospheric variables at different pressure levels (in the simplified version, and many others in the original), the AQFM groups variables into three categories:\n",
                "- Sensor readings (PT08.S* series)\n",
                "- Ground truth measurements (*GT series)\n",
                "- Physical parameters (temperature, relative humidity, absolute humidity)\n",
                "\n",
                "### 2. Embedding Strategy\n",
                "The BFM used patch-based embeddings:\n",
                "```python\n",
                "self.surface_embed = nn.Linear(len(surface_vars) * patch_size * patch_size, embed_dim)\n",
                "```\n",
                "which processed spatial patches of each variable type. In contrast, the AQFM uses direct linear projections:\n",
                "```python\n",
                "self.sensor_embed = nn.Linear(len(feature_names['sensor']), embed_dim)\n",
                "```\n",
                "This simpler approach is possible because we're dealing with 1D temporal sequences rather than 2D spatial grids.\n",
                "\n",
                "### 3. Position Encoding\n",
                "The BFM needed to encode both latitude and longitude positions:\n",
                "```python\n",
                "self.pos_embed = nn.Linear(2, embed_dim) # lat, lon\n",
                "```\n",
                "and created a complex position grid using meshgrid. The AQFM's position encoding is handled entirely by the Perceiver IO's built-in Fourier features, which are better suited for temporal sequences. This is why we specify:\n",
                "```python\n",
                "position_encoding_type=\"fourier\",\n",
                "num_fourier_bands=32,\n",
                "max_frequency=max_history_size,\n",
                "num_input_axes=1 # temporal dimension only\n",
                "```\n",
                "### 4. Variable Combination\n",
                "In the BFM, variables were processed separately and then concatenated:\n",
                "```python\n",
                "embeddings = []\n",
                "if surface_embed is not None:\n",
                "  embeddings.append(surface_embed)\n",
                "if atmos_embeds:\n",
                "  embeddings.append(torch.cat(atmos_embeds, dim=0))\n",
                "x = torch.cat(embeddings, dim=0)\n",
                "```\n",
                "The AQFM uses addition instead:\n",
                "```python\n",
                "combined_embed = sensor_embed + ground_truth_embed + physical_embed\n",
                "```\n",
                "\n",
                "This additive approach helps maintain the temporal alignment of different variable types and allows for better interaction between different measurement types at the same timestep.\n",
                "\n",
                "### 5. Perceiver IO Configuration\n",
                "While both versions use Perceiver IO as their core processing component, the configurations differ:\n",
                "- BFM focused on spatial relationships with attention heads processing patches + the patches' various version at different time steps\n",
                "- AQFM emphasizes temporal relationships with attention heads processing time steps\n",
                "- Both maintain the concept of latent tokens, but they represent different things:\n",
                "  * BFM: spatial-temporal patterns across the globe\n",
                "  * AQFM: temporal patterns and variable interactions in the air quality measurements (in one singular location, if one may wish to think of it that way)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <span style=\"color: orange;\">AQFM Decoder</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleAQDecoder(nn.Module):\n",
                "    def __init__(self, feature_names, embed_dim, num_heads=8, head_dim=64, depth=2, drop_rate=0.1):\n",
                "        super().__init__()\n",
                "        self.feature_names = feature_names\n",
                "        self.embed_dim = embed_dim\n",
                "        \n",
                "        # create clean target names mapping\n",
                "        self.target_names = {\n",
                "            name: f\"target_{i}\" \n",
                "            for i, name in enumerate(feature_names[\"ground_truth\"])\n",
                "        }\n",
                "        \n",
                "        # initialize perceiver IO for decoding\n",
                "        self.perceiver = PerceiverIO(\n",
                "            num_layers=depth,\n",
                "            dim=embed_dim,\n",
                "            queries_dim=embed_dim,\n",
                "            logits_dimension=None,\n",
                "            num_latent_tokens=len(feature_names[\"ground_truth\"]),  # one token per target\n",
                "            latent_dimension=embed_dim,\n",
                "            cross_attention_heads=num_heads,\n",
                "            latent_attention_heads=num_heads,\n",
                "            cross_attention_head_dim=head_dim,\n",
                "            latent_attention_head_dim=head_dim,\n",
                "            sequence_dropout_prob=drop_rate,\n",
                "            num_fourier_bands=32,\n",
                "            max_frequency=24,  # for 24 hours of history\n",
                "            num_input_axes=1,\n",
                "            position_encoding_type=\"fourier\",\n",
                "        )\n",
                "        \n",
                "        # prediction heads for each target variable\n",
                "        self.prediction_heads = nn.ModuleDict({\n",
                "            self.target_names[name]: nn.Sequential(\n",
                "                nn.Linear(embed_dim, embed_dim // 2),\n",
                "                nn.ReLU(),\n",
                "                nn.Dropout(drop_rate),\n",
                "                nn.Linear(embed_dim // 2, embed_dim // 4),\n",
                "                nn.ReLU(),\n",
                "                nn.Dropout(drop_rate),\n",
                "                nn.Linear(embed_dim // 4, 1),  # single value prediction\n",
                "            )\n",
                "            for name in feature_names[\"ground_truth\"]\n",
                "        })\n",
                "        \n",
                "        # query tokens for each target variable\n",
                "        self.query_tokens = nn.Parameter(\n",
                "            torch.randn(len(feature_names[\"ground_truth\"]), embed_dim)\n",
                "        )\n",
                "        \n",
                "        # lead time embedding\n",
                "        self.lead_time_embed = nn.Linear(1, embed_dim)\n",
                "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
                "        \n",
                "    def forward(self, x, batch, lead_time):\n",
                "        B = x.shape[0]  # shape: [b, num_latent_tokens, embed_dim]\n",
                "        device = x.device\n",
                "        \n",
                "        # prepare queries\n",
                "        queries = repeat(self.query_tokens, \"n d -> b n d\", b=B)\n",
                "        \n",
                "        # add lead time information to queries\n",
                "        lead_hours = torch.tensor(\n",
                "            [[lead_time.total_seconds() / 3600]], \n",
                "            device=device, \n",
                "            dtype=torch.float\n",
                "        )\n",
                "        lead_time_encoding = self.lead_time_embed(lead_hours)\n",
                "        queries = queries + lead_time_encoding.unsqueeze(1)\n",
                "        \n",
                "        # process through perceiver IO\n",
                "        decoded = self.perceiver(x, queries=queries)\n",
                "        decoded = self.pos_drop(decoded)\n",
                "        \n",
                "        # generate predictions for each target variable\n",
                "        predictions = {}\n",
                "        for idx, (orig_name, safe_name) in enumerate(self.target_names.items()):\n",
                "            token_embedding = decoded[:, idx]  # shape: [B, embed_dim]\n",
                "            pred = self.prediction_heads[safe_name](token_embedding)  # shape: [B, 1]\n",
                "            predictions[orig_name] = pred\n",
                "            \n",
                "        return predictions  # return only ground truth predictions"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As you can see, the transition from BFM's spatial-temporal predictions to AQFM's temporal air quality forecasting required quite a dozen of changes in the decoder architecture. Let's analyze these adaptations and what could one do to adapt their own decoder to some new data formats:\n",
                "\n",
                "### 1. Output Structure and Prediction Targets\n",
                "The BFM decoder needed to reconstruct complete spatial fields:\n",
                "- Surface variables (2D fields: [B, V, H, W]);\n",
                "- Atmospheric variables (3D fields: [B, V, L, H, W]);\n",
                "\n",
                "In contrast, the AQFM decoder focuses on point predictions:\n",
                "- Only ground truth variables;\n",
                "- Single value per variable: [B, 1];\n",
                "- No spatial reconstruction needed;\n",
                "\n",
                "This fundamental difference led to much simpler output projections in AQFM:\n",
                "```python\n",
                "# BFM's spatial projections\n",
                "self.surface_proj = nn.Linear(embed_dim, len(surface_vars))\n",
                "self.atmos_proj = nn.Linear(embed_dim, len(atmos_vars))\n",
                "\n",
                "# AQFM's prediction heads\n",
                "self.prediction_heads = nn.ModuleDict({\n",
                "    self.target_names[name]: nn.Sequential(\n",
                "        nn.Linear(embed_dim, embed_dim // 2),\n",
                "        nn.ReLU(),\n",
                "        nn.Dropout(drop_rate),\n",
                "        nn.Linear(embed_dim // 2, embed_dim // 4),\n",
                "        nn.ReLU(),\n",
                "        nn.Dropout(drop_rate),\n",
                "        nn.Linear(embed_dim // 4, 1),\n",
                "    )\n",
                "    for name in feature_names[\"ground_truth\"]\n",
                "})\n",
                "```\n",
                "<span style=\"color: orange;\">**NOTE**</span>: We can actually leverage Perceiver IO's query system directly for predictions, as demonstrated in `perceiver_io_prediction.py`. Here's how:\n",
                "\n",
                "1. Instead of using prediction heads, we can define learnable query tokens:\n",
                "```python\n",
                "self.query = nn.Parameter(torch.randn(1, queries_dim))\n",
                "```\n",
                "\n",
                "2. Configure Perceiver IO to output the exact dimensions we need:\n",
                "```python\n",
                "self.perceiver_io = PerceiverIO(\n",
                "    # ... other params ...\n",
                "    logits_dimension=1,  # Direct prediction output\n",
                "    queries_dim=queries_dim,\n",
                "    # ... other params ...\n",
                ")\n",
                "```\n",
                "\n",
                "3. Use the query in the forward pass:\n",
                "```python\n",
                "def forward(self, x):\n",
                "    batch_size = x.shape[0]\n",
                "    queries = self.query.expand(batch_size, 1, -1)\n",
                "    return self.perceiver_io(x, queries=queries)\n",
                "```\n",
                "\n",
                "### 2. Query System\n",
                "Both decoders use queries to extract information from the encoded representation, but with different purposes:\n",
                "\n",
                "BFM:\n",
                "- One query per spatial patch;\n",
                "- Position-based queries using lat/lon coordinates;\n",
                "- Queries aim to reconstruct spatial patterns;\n",
                "\n",
                "AQFM:\n",
                "- One query per target variable;\n",
                "- Learnable query tokens;\n",
                "- Queries focus on temporal patterns specific to each variable;\n",
                "\n",
                "### 3. Position and Time Encoding\n",
                "BFM needed explicit spatial position encoding:\n",
                "```python\n",
                "pos = torch.stack(torch.meshgrid(\n",
                "    torch.linspace(-1, 1, self.num_patches_h),\n",
                "    torch.linspace(-1, 1, self.num_patches_w),\n",
                "    indexing=\"ij\"\n",
                "), dim=-1)\n",
                "```\n",
                "\n",
                "AQFM uses:\n",
                "- Fourier features for temporal encoding;\n",
                "- Lead time embedding added to queries;\n",
                "- No need for spatial position encoding;\n",
                "\n",
                "### 4. Output Processing\n",
                "BFM required complex output processing:\n",
                "- Reshape from patches to full fields;\n",
                "- Interpolation to original resolution;\n",
                "- Handling multiple pressure levels;\n",
                "\n",
                "AQFM's output processing is straightforward:\n",
                "- Direct projection to scalar values;\n",
                "- No spatial reconstruction;\n",
                "- No need for interpolation;\n",
                "\n",
                "### 5. Perceiver IO Configuration\n",
                "While both use Perceiver IO, their configurations reflect different goals:\n",
                "\n",
                "BFM:\n",
                "- Queries based on spatial positions;\n",
                "- Focus on spatial reconstruction;\n",
                "- Simple cross-attention setup;\n",
                "\n",
                "AQFM:\n",
                "- Learnable query tokens;\n",
                "- Emphasis on temporal patterns;\n",
                "- More sophisticated prediction heads;\n",
                "\n",
                "### 6. Initialization\n",
                "AQFM uses:\n",
                "- MLPs for each target;\n",
                "- Separate processing paths for each variable;\n",
                "\n",
                "BFM uses:\n",
                "- Truncated normal initialization;\n",
                "- Simple linear projections;\n",
                "- Shared processing for variable types;\n",
                "\n",
                "These adaptations reflect the fundamental shift from spatial field reconstruction to temporal point prediction, resulting in a more focused and efficient architecture for air quality forecasting."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <span style=\"color: orange;\">AQFM</span>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class AQMetadata:\n",
                "    \"\"\"Metadata for air quality data, just like climate data\"\"\"\n",
                "\n",
                "    time: List[datetime]\n",
                "    feature_names: Dict[str, List[str]]\n",
                "    sequence_length: int\n",
                "    prediction_horizon: int\n",
                "\n",
                "\n",
                "@dataclass\n",
                "class AQBatch:\n",
                "    \"\"\"Batch structure for air quality data, again just like climate data\"\"\"\n",
                "\n",
                "    sensor_vars: Dict[str, torch.Tensor]  # PT08.S* sensor readings\n",
                "    ground_truth_vars: Dict[str, torch.Tensor]  # ground truth ones (*GT)\n",
                "    physical_vars: Dict[str, torch.Tensor]  # any other things like T, RH, AH\n",
                "    metadata: AQMetadata"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleAQFM(nn.Module):\n",
                "    def __init__(\n",
                "        self,\n",
                "        feature_names: dict[str, list[str]],\n",
                "        embed_dim: int = 512,  # increased to match original\n",
                "        num_latent_tokens: int = 8,\n",
                "        max_history_size: int = 24,\n",
                "        backbone_type: Literal[\"swin\", \"mvit\", \"identity\"] = \"identity\",\n",
                "        # encoder params\n",
                "        encoder_num_heads: int = 8,\n",
                "        encoder_head_dim: int = 64,\n",
                "        encoder_depth: int = 2,\n",
                "        encoder_drop_rate: float = 0.1,\n",
                "        # decoder params\n",
                "        decoder_num_heads: int = 8,\n",
                "        decoder_head_dim: int = 64,\n",
                "        decoder_depth: int = 2,\n",
                "        decoder_drop_rate: float = 0.1,\n",
                "    ):\n",
                "        super().__init__()\n",
                "        \n",
                "        # initialize encoder with more parameters\n",
                "        self.encoder = SimpleAQEncoder(\n",
                "            feature_names=feature_names,\n",
                "            embed_dim=embed_dim,\n",
                "            num_latent_tokens=num_latent_tokens,\n",
                "            max_history_size=max_history_size,\n",
                "            num_heads=encoder_num_heads,\n",
                "            head_dim=encoder_head_dim,\n",
                "            depth=encoder_depth,\n",
                "            drop_rate=encoder_drop_rate,\n",
                "        )\n",
                "        \n",
                "        # initialize backbone based on type\n",
                "        if backbone_type == \"identity\":\n",
                "            self.backbone = nn.Identity()\n",
                "        elif backbone_type == \"swin\":\n",
                "            pass # for simplicity, we don't use swin\n",
                "        elif backbone_type == \"mvit\":\n",
                "            pass # and also mvit\n",
                "        \n",
                "        # initialize decoder with more parameters\n",
                "        self.decoder = SimpleAQDecoder(\n",
                "            feature_names=feature_names,\n",
                "            embed_dim=embed_dim,\n",
                "            num_heads=decoder_num_heads,\n",
                "            head_dim=decoder_head_dim,\n",
                "            depth=decoder_depth,\n",
                "            drop_rate=decoder_drop_rate,\n",
                "        )\n",
                "        \n",
                "        self.backbone_type = backbone_type\n",
                "\n",
                "    def forward(self, batch, lead_time):\n",
                "        # encode\n",
                "        encoded = self.encoder(batch, lead_time)\n",
                "        \n",
                "        # process through backbone\n",
                "        if self.backbone_type in [\"swin\", \"mvit\"]:\n",
                "            patch_shape = [self.encoder.num_latent_tokens, 1, 1]\n",
                "            processed = self.backbone(encoded, lead_time=lead_time, \n",
                "                                   rollout_step=0, patch_shape=patch_shape)\n",
                "        else:\n",
                "            processed = self.backbone(encoded)\n",
                "        \n",
                "        # decode\n",
                "        output = self.decoder(processed, batch, lead_time)\n",
                "        \n",
                "        return output"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As for the whole model, where we integrate all the components into one class definition, it's pretty much the same as the SimpleBFM, only that we now have a different encoder and decoder. The same statement and general concepts extends to full implementations in `aqfm.py` and `bfm.py`.\t\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Model outputs:\n",
                        "  - CO(GT): torch.Size([1, 1])\n",
                        "  - NMHC(GT): torch.Size([1, 1])\n",
                        "  - C6H6(GT): torch.Size([1, 1])\n",
                        "  - NOx(GT): torch.Size([1, 1])\n",
                        "  - NO2(GT): torch.Size([1, 1])\n"
                    ]
                }
            ],
            "source": [
                "def create_sample_aq_batch():\n",
                "    # define feature names\n",
                "    feature_names = {\n",
                "        \"sensor\": [\"PT08.S1(CO)\", \"PT08.S2(NMHC)\", \"PT08.S3(NOx)\", \"PT08.S4(NO2)\", \"PT08.S5(O3)\"],\n",
                "        \"ground_truth\": [\"CO(GT)\", \"NMHC(GT)\", \"C6H6(GT)\", \"NOx(GT)\", \"NO2(GT)\"],\n",
                "        \"physical\": [\"T\", \"RH\", \"AH\"],\n",
                "    }\n",
                "    \n",
                "    # create random tensors for each feature group with batch dimension\n",
                "    sensor_vars = {name: torch.rand(1, 24) for name in feature_names[\"sensor\"]}  # shape: [B, T]\n",
                "    ground_truth_vars = {name: torch.rand(1, 24) for name in feature_names[\"ground_truth\"]}  # shape: [B, T]\n",
                "    physical_vars = {name: torch.rand(1, 24) for name in feature_names[\"physical\"]}  # shape: [B, T]\n",
                "    \n",
                "    # create metadata\n",
                "    metadata = AQMetadata(\n",
                "        time=[datetime.now() for _ in range(24)],\n",
                "        feature_names=feature_names,\n",
                "        sequence_length=24,\n",
                "        prediction_horizon=1,\n",
                "    )\n",
                "    \n",
                "    # create a batch\n",
                "    batch = AQBatch(\n",
                "        sensor_vars=sensor_vars,\n",
                "        ground_truth_vars=ground_truth_vars,\n",
                "        physical_vars=physical_vars,\n",
                "        metadata=metadata\n",
                "    )\n",
                "    \n",
                "    return batch\n",
                "\n",
                "def test_simple_aqfm():\n",
                "    # create model\n",
                "    feature_names = {\n",
                "        \"sensor\": [\"PT08.S1(CO)\", \"PT08.S2(NMHC)\", \"PT08.S3(NOx)\", \"PT08.S4(NO2)\", \"PT08.S5(O3)\"],\n",
                "        \"ground_truth\": [\"CO(GT)\", \"NMHC(GT)\", \"C6H6(GT)\", \"NOx(GT)\", \"NO2(GT)\"],\n",
                "        \"physical\": [\"T\", \"RH\", \"AH\"],\n",
                "    }\n",
                "    model = SimpleAQFM(feature_names=feature_names)\n",
                "    \n",
                "    # create sample batch\n",
                "    batch = create_sample_aq_batch()\n",
                "    lead_time = timedelta(hours=1)\n",
                "    \n",
                "    # process batch\n",
                "    try:\n",
                "        predictions = model(batch, lead_time)\n",
                "        \n",
                "        print(\"\\nModel outputs:\")\n",
                "        for var_name, pred in predictions.items():\n",
                "            print(f\"  - {var_name}: {pred.shape}\")\n",
                "                \n",
                "    except Exception as e:\n",
                "        print(f\"\\nError in processing:\")\n",
                "        print(f\"Error message: {str(e)}\")\n",
                "        raise e\n",
                "\n",
                "test_simple_aqfm()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
