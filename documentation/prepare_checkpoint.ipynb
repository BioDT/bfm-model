{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "import torch\n",
    "from safetensors.torch import (\n",
    "    save_file as save_safetensors,\n",
    "    load_file as load_safetensors,\n",
    ")\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_KEYS: set[str] = {\n",
    "    \"optimizer_states\",\n",
    "    \"lr_schedulers\",\n",
    "    \"callbacks\",\n",
    "    \"loops\",\n",
    "    \"amp_scaler\",\n",
    "    # keep hyper_parameters optionally\n",
    "}\n",
    "\n",
    "def _dedupe_state_dict(sd: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Clone tensors that share storage so safetensors can serialize them safely.\"\"\"\n",
    "    seen: dict[int, str] = {}\n",
    "    deduped: dict[str, torch.Tensor] = {}\n",
    "\n",
    "    for name, tensor in sd.items():\n",
    "        ptr = tensor.storage().data_ptr()\n",
    "        if ptr in seen:\n",
    "            deduped[name] = tensor.clone()\n",
    "            print(f\" Cloned shared tensor '{name}' (alias of '{seen[ptr]}')\")\n",
    "        else:\n",
    "            deduped[name] = tensor\n",
    "            seen[ptr] = name\n",
    "    return deduped\n",
    "\n",
    "def _validate_state_dict(original_sd: Dict[str, torch.Tensor], saved_file: Path) -> None:\n",
    "    \"\"\"Byte-level integrity check between in-memory and on-disk weights.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If a key is missing/extra or any tensor differs in shape, dtype or value.\n",
    "    \"\"\"\n",
    "    loaded_sd: Dict[str, torch.Tensor] = load_safetensors(str(saved_file))\n",
    "\n",
    "    if original_sd.keys() != loaded_sd.keys():\n",
    "        missing = original_sd.keys() - loaded_sd.keys()\n",
    "        extra = loaded_sd.keys() - original_sd.keys()\n",
    "        raise ValueError(\n",
    "            f\"Key mismatch between in-memory and saved weights. Missing: {missing}, Extra: {extra}\"\n",
    "        )\n",
    "\n",
    "    for k, t in original_sd.items():\n",
    "        l = loaded_sd[k]\n",
    "        if t.shape != l.shape or t.dtype != l.dtype or not torch.equal(t.cpu(), l.cpu()):\n",
    "            raise ValueError(f\"Tensor mismatch for key '{k}'\")\n",
    "\n",
    "    print(\"Validation passed: saved weights are byte-identical to the in-memory state_dict\")\n",
    "\n",
    "\n",
    "def _strip_checkpoint(\n",
    "    ckpt_path: Path,\n",
    "    output_dir: Path,\n",
    "    keep_hparams: bool = True,\n",
    "    validate: bool = True,\n",
    ") -> Path:\n",
    "    \"\"\"Load ckpt_path, drop non-essential keys, export as .safetensors, validate.\"\"\"\n",
    "    ckpt: Dict[str, Any] = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "    state_dict = _dedupe_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "    minimal_state: Dict[str, Any] = {\"state_dict\": state_dict}\n",
    "    if keep_hparams and \"hyper_parameters\" in ckpt:\n",
    "        minimal_state[\"hyper_parameters\"] = ckpt[\"hyper_parameters\"]\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    target = output_dir / f\"{ckpt_path.stem}_minimal.safetensors\"\n",
    "    save_safetensors(state_dict, str(target))\n",
    "\n",
    "    if validate:\n",
    "        _validate_state_dict(state_dict, target)\n",
    "\n",
    "    if keep_hparams and \"hyper_parameters\" in minimal_state:\n",
    "        with open(output_dir / \"hparams.json\", \"w\", encoding=\"utf-8\") as fh:\n",
    "            json.dump(minimal_state[\"hyper_parameters\"], fh, indent=2)\n",
    "\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_checkpoint(\n",
    "    ckpt_path: str | Path,\n",
    "    *,\n",
    "    keep_hparams: bool = False,\n",
    "    validate: bool = True,\n",
    "    backend: str = \"torch\",\n",
    ") -> Path:\n",
    "    \"\"\"Strip training artefacts from a Lightning .ckpt file and save <stem>_clean.*.\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        Path to the cleaned weights file.\n",
    "    \"\"\"\n",
    "    ckpt_path = Path(ckpt_path)\n",
    "    ckpt: Dict[str, Any] = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "    # prune trainingâ€‘only sections\n",
    "    # state_dict = _dedupe_state_dict(ckpt[\"state_dict\"])\n",
    "    state_dict = ckpt[\"state_dict\"]\n",
    "\n",
    "    if backend == \"safetensors\":\n",
    "        out_file = ckpt_path.with_name(f\"{ckpt_path.stem}_clean.safetensors\")\n",
    "        save_safetensors(state_dict, str(out_file))\n",
    "        if validate:\n",
    "            _validate_state_dict(state_dict, out_file)\n",
    "    elif backend == \"torch\":\n",
    "        out_file = ckpt_path.with_name(f\"{ckpt_path.stem}_clean{ckpt_path.suffix}\")\n",
    "        torch.save({\"state_dict\": state_dict}, out_file)\n",
    "    else:\n",
    "        raise ValueError(\"backend must be 'safetensors' or 'torch'\")\n",
    "\n",
    "    if keep_hparams and \"hyper_parameters\" in ckpt:\n",
    "        (out_file.parent / \"hparams.json\").write_text(\n",
    "            json.dumps(ckpt[\"hyper_parameters\"], indent=2),\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "\n",
    "    print(f\"Cleaned checkpoint saved to {out_file}\")\n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just clean a checkpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CKPT = Path(\"your_weights.ckpt\")\n",
    "\n",
    "clean_ckpt = clean_checkpoint(INPUT_CKPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for HF upload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CKPT = Path(\"your_weights.ckpt\")\n",
    "OUTPUT_DIR = Path(\"your_weights_folder/clean\")\n",
    "\n",
    "cleaned = _strip_checkpoint(ckpt_path=INPUT_CKPT, output_dir=OUTPUT_DIR, keep_hparams=False)\n",
    "print(f\"Saved minimal weights: {cleaned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_w = load_file(\"your_weights.safetensors\")\n",
    "cleaned_w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
