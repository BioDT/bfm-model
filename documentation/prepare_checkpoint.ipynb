{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "import torch\n",
    "from huggingface_hub import HfApi, HfFolder, create_repo, upload_folder\n",
    "from safetensors.torch import (\n",
    "    save_file as save_safetensors,\n",
    "    load_file as load_safetensors,\n",
    ")\n",
    "from safetensors.torch import save_model, load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRAINING_KEYS: set[str] = {\n",
    "    \"optimizer_states\",\n",
    "    \"lr_schedulers\",\n",
    "    \"callbacks\",\n",
    "    \"loops\",\n",
    "    \"amp_scaler\",\n",
    "    # keep hyper_parameters optionally\n",
    "}\n",
    "\n",
    "def _dedupe_state_dict(sd: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Clone tensors that share storage so safetensors can serialize them safely.\"\"\"\n",
    "    seen: dict[int, str] = {}\n",
    "    deduped: dict[str, torch.Tensor] = {}\n",
    "\n",
    "    for name, tensor in sd.items():\n",
    "        ptr = tensor.storage().data_ptr()\n",
    "        if ptr in seen:\n",
    "            deduped[name] = tensor.clone()\n",
    "            print(f\" Cloned shared tensor '{name}' (alias of '{seen[ptr]}')\")\n",
    "        else:\n",
    "            deduped[name] = tensor\n",
    "            seen[ptr] = name\n",
    "    return deduped\n",
    "\n",
    "def _validate_state_dict(original_sd: Dict[str, torch.Tensor], saved_file: Path) -> None:\n",
    "    \"\"\"Byte-level integrity check between in-memory and on-disk weights.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If a key is missing/extra or any tensor differs in shape, dtype or value.\n",
    "    \"\"\"\n",
    "    loaded_sd: Dict[str, torch.Tensor] = load_safetensors(str(saved_file))  # type: ignore[arg-type]\n",
    "\n",
    "    if original_sd.keys() != loaded_sd.keys():\n",
    "        missing = original_sd.keys() - loaded_sd.keys()\n",
    "        extra = loaded_sd.keys() - original_sd.keys()\n",
    "        raise ValueError(\n",
    "            f\"Key mismatch between in-memory and saved weights. Missing: {missing}, Extra: {extra}\"\n",
    "        )\n",
    "\n",
    "    for k, t in original_sd.items():\n",
    "        l = loaded_sd[k]\n",
    "        if t.shape != l.shape or t.dtype != l.dtype or not torch.equal(t.cpu(), l.cpu()):\n",
    "            raise ValueError(f\"Tensor mismatch for key '{k}'\")\n",
    "\n",
    "    print(\"Validation passed: saved weights are byte-identical to the in-memory state_dict\")\n",
    "\n",
    "\n",
    "def _strip_checkpoint(\n",
    "    ckpt_path: Path,\n",
    "    output_dir: Path,\n",
    "    keep_hparams: bool = True,\n",
    "    validate: bool = True,\n",
    ") -> Path:\n",
    "    \"\"\"Load ckpt_path, drop non-essential keys, export as .safetensors, validate.\"\"\"\n",
    "    ckpt: Dict[str, Any] = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "    state_dict = _dedupe_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "    minimal_state: Dict[str, Any] = {\"state_dict\": state_dict}\n",
    "    if keep_hparams and \"hyper_parameters\" in ckpt:\n",
    "        minimal_state[\"hyper_parameters\"] = ckpt[\"hyper_parameters\"]\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    target = output_dir / f\"{ckpt_path.stem}_minimal.safetensors\"\n",
    "    save_safetensors(state_dict, str(target))\n",
    "\n",
    "    if validate:\n",
    "        _validate_state_dict(state_dict, target)\n",
    "\n",
    "    if keep_hparams and \"hyper_parameters\" in minimal_state:\n",
    "        with open(output_dir / \"hparams.json\", \"w\", encoding=\"utf-8\") as fh:\n",
    "            json.dump(minimal_state[\"hyper_parameters\"], fh, indent=2)\n",
    "\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CKPT = Path(\"/projects/prjs1134/data/projects/biodt/storage/runs_storage/19-01-40/checkpoints/epoch=419-val_loss=0.01776.ckpt\")\n",
    "OUTPUT_DIR = Path(\"/projects/prjs1134/data/projects/biodt/storage/weights/cleaned\")\n",
    "\n",
    "cleaned = _strip_checkpoint(ckpt_path=INPUT_CKPT, output_dir=OUTPUT_DIR, keep_hparams=False)\n",
    "print(f\"Saved minimal weights: {cleaned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_w = load_file(\"/projects/prjs1134/data/projects/biodt/storage/weights/cleaned/epoch=419-val_loss=0.01776_minimal.safetensors\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
