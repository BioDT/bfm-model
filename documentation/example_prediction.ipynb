{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atrantas/production/bfm-model/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "GlobalHydra.instance().clear()\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from safetensors.torch import save_model, load_file\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from bfm_model.bfm.dataloader_monthly import LargeClimateDataset\n",
    "from bfm_model.bfm.model_helpers import get_mlflow_logger, get_trainer, setup_bfm_model\n",
    "from bfm_model.bfm.dataloader_helpers import get_val_dataloader\n",
    "from bfm_model.bfm.utils import plot_europe_timesteps_and_difference\n",
    "\n",
    "hydra.initialize(config_path=\"../bfm_model/bfm/configs\", version_base=None)\n",
    "cfg = hydra.compose(config_name=\"train_config\", overrides=[\"model.embed_dim=512\", \"model.depth=6\",\n",
    "\"model.swin_backbone_size=medium\", \"training.devices=[0]\"]\n",
    "    )\n",
    "\n",
    "# print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "# LOAD THE MODEL \n",
    "checkpoint_repo = \"BioDT/bfm-pretrained\"\n",
    "dcheckpoint_name = \"bfm-pretrain-medium.safetensors\"\n",
    "\n",
    "checkpoint_path = hf_hub_download(repo_id=checkpoint_repo, filename=dcheckpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We scale the dataset True with normalize\n",
      "We scale the dataset True with normalize\n",
      "Validation train: 13\n",
      "Land-sea mask file not found at . Loss will be calculated over all pixels.\n",
      "Num of patches in Encoder: 2800\n",
      "Total latens 64400\n",
      "BuiltinAttention q_dim 512 | context dim 641 | num q heads 16 | head dim 64 | kv_heads 8\n",
      "BuiltinAttention q_dim 512 | context dim 512 | num q heads 16 | head dim 64 | kv_heads 8\n",
      "BuiltinAttention q_dim 512 | context dim 512 | num q heads 16 | head dim 64 | kv_heads 8\n",
      "BuiltinAttention q_dim 512 | context dim 512 | num q heads 16 | head dim 64 | kv_heads 8\n",
      "BuiltinAttention q_dim 512 | context dim 512 | num q heads 16 | head dim 64 | kv_heads 8\n",
      "BuiltinAttention q_dim 512 | context dim 512 | num q heads 16 | head dim 64 | kv_heads 8\n",
      "BuiltinAttention q_dim 512 | context dim 512 | num q heads 16 | head dim 64 | kv_heads 8\n",
      "BuiltinAttention q_dim 512 | context dim 512 | num q heads 16 | head dim 64 | kv_heads 8\n",
      "Total query tokens for Decoder:  124\n",
      "BuiltinAttention q_dim 512 | context dim 641 | num q heads 16 | head dim 64 | kv_heads 8\n",
      "BuiltinAttention q_dim 512 | context dim 512 | num q heads 16 | head dim 64 | kv_heads 8\n",
      "BuiltinAttention q_dim 512 | context dim 512 | num q heads 16 | head dim 64 | kv_heads 8\n",
      "BuiltinAttention q_dim 512 | context dim 512 | num q heads 16 | head dim 64 | kv_heads 8\n",
      "BuiltinAttention q_dim 512 | context dim 512 | num q heads 16 | head dim 64 | kv_heads 8\n",
      "BuiltinAttention q_dim 512 | context dim 512 | num q heads 16 | head dim 64 | kv_heads 8\n",
      "BuiltinAttention q_dim 512 | context dim 512 | num q heads 16 | head dim 64 | kv_heads 8\n",
      "BuiltinAttention q_dim 512 | context dim 512 | num q heads 16 | head dim 64 | kv_heads 8\n",
      "   | Name                             | Type              | Params | Mode \n",
      "--------------------------------------------------------------------------------\n",
      "0  | encoder                          | BFMEncoder        | 102 M  | train\n",
      "1  | encoder.pos_embed                | Linear            | 132 K  | train\n",
      "2  | encoder.lead_time_embed          | Linear            | 262 K  | train\n",
      "3  | encoder.absolute_time_embed      | Linear            | 262 K  | train\n",
      "4  | encoder.atmos_levels_embed       | Embedding         | 6.7 K  | train\n",
      "5  | encoder.surface_token_embeds     | Linear            | 115 K  | train\n",
      "6  | encoder.edaphic_token_embeds     | Linear            | 66.0 K | train\n",
      "7  | encoder.atmos_token_embeds       | Linear            | 82.4 K | train\n",
      "8  | encoder.climate_token_embeds     | Linear            | 180 K  | train\n",
      "9  | encoder.species_token_embeds     | Linear            | 459 K  | train\n",
      "10 | encoder.vegetation_token_embeds  | Linear            | 16.9 K | train\n",
      "11 | encoder.land_token_embeds        | Linear            | 16.9 K | train\n",
      "12 | encoder.agriculture_token_embeds | Linear            | 49.7 K | train\n",
      "13 | encoder.forest_token_embeds      | Linear            | 16.9 K | train\n",
      "14 | encoder.redlist_token_embeds     | Linear            | 16.9 K | train\n",
      "15 | encoder.misc_token_embeds        | Linear            | 33.3 K | train\n",
      "16 | encoder.pos_drop                 | Dropout           | 0      | train\n",
      "17 | encoder.pre_perceiver_norm       | LayerNorm         | 1.0 K  | train\n",
      "18 | encoder._latent_parameter_list   | ParameterList     | 33.0 M | train\n",
      "19 | encoder.perceiver_io             | PerceiverIO       | 67.8 M | train\n",
      "20 | backbone                         | Swin3DTransformer | 81.6 M | train\n",
      "21 | backbone.time_mlp                | Sequential        | 525 K  | train\n",
      "22 | backbone.encoder_layers          | ModuleList        | 40.4 M | train\n",
      "23 | backbone.decoder_layers          | ModuleList        | 40.1 M | train\n",
      "24 | backbone.final_proj              | Linear            | 524 K  | train\n",
      "25 | decoder                          | BFMDecoder        | 288 M  | train\n",
      "26 | decoder.pos_embed                | Linear            | 67.6 K | train\n",
      "27 | decoder.lead_time_embed          | Linear            | 262 K  | train\n",
      "28 | decoder.absolute_time_embed      | Linear            | 262 K  | train\n",
      "29 | decoder.surface_token_proj       | Linear            | 23.0 M | train\n",
      "30 | decoder.edaphic_token_proj       | Linear            | 23.0 M | train\n",
      "31 | decoder.atmos_token_proj         | Linear            | 23.0 M | train\n",
      "32 | decoder.climate_token_proj       | Linear            | 23.0 M | train\n",
      "33 | decoder.species_token_proj       | Linear            | 23.0 M | train\n",
      "34 | decoder.vegetation_token_proj    | Linear            | 23.0 M | train\n",
      "35 | decoder.land_token_proj          | Linear            | 23.0 M | train\n",
      "36 | decoder.agriculture_token_proj   | Linear            | 23.0 M | train\n",
      "37 | decoder.forest_token_proj        | Linear            | 23.0 M | train\n",
      "38 | decoder.redlist_token_proj       | Linear            | 23.0 M | train\n",
      "39 | decoder.misc_token_proj          | Linear            | 23.0 M | train\n",
      "40 | decoder.perceiver_io             | PerceiverIO       | 34.9 M | train\n",
      "41 | decoder.pos_drop                 | Dropout           | 0      | train\n",
      "--------------------------------------------------------------------------------\n",
      "472 M     Trainable params\n",
      "0         Non-trainable params\n",
      "472 M     Total params\n",
      "1,889.283 Total estimated model params size (MB)\n",
      "483       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Done \n",
      " Setting up the BFM model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atrantas/production/bfm-model/venv/lib/python3.12/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/atrantas/production/bfm-model/venv/lib/python3 ...\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/atrantas/production/bfm-model/venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA H100') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 13/13 [00:07<00:00,  1.75it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = LargeClimateDataset(\n",
    "    data_dir=cfg.data.test_data_path, # Adapt that to your folder that contains the batches\n",
    "    scaling_settings=cfg.data.scaling,\n",
    "    num_species=cfg.data.species_number,\n",
    "    atmos_levels=cfg.data.atmos_levels,\n",
    "    model_patch_size=cfg.model.patch_size,\n",
    ")\n",
    "# override batch_size\n",
    "test_dataloader = get_val_dataloader(cfg, batch_size_override=cfg.evaluation.batch_size)\n",
    "\n",
    "\n",
    "bfm_model = setup_bfm_model(cfg, mode=\"test\")\n",
    "\n",
    "# When you have a local checkpoint path.\n",
    "# bfm_model.load_state_dict(path, strict=False)\n",
    "\n",
    "# When you load from HF\n",
    "state_dict = load_file(checkpoint_path)\n",
    "\n",
    "bfm_model.eval()\n",
    "bfm_model.to(\"cuda\")\n",
    "\n",
    "trainer = get_trainer(\n",
    "    cfg,\n",
    "    mlflow_logger=None,\n",
    "    callbacks=[],\n",
    ")\n",
    "\n",
    "predictions = trainer.predict(model=bfm_model, dataloaders=test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2020-05-01 00:00:00',), ('2020-06-01 00:00:00',)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_prediction = predictions[-1][0][\"pred\"]\n",
    "last_ground_truth = predictions[-1][0][\"gt\"]\n",
    "\n",
    "last_ground_truth.batch_metadata.timestamp\n",
    "# Thus the prediction is 1 month ahead -> 2020-07-01 00:00:00"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
