{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "GlobalHydra.instance().clear()\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from safetensors.torch import save_model, load_file\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from bfm_model.bfm.dataloader_monthly import LargeClimateDataset\n",
    "from bfm_model.bfm.model_helpers import get_trainer, setup_bfm_model\n",
    "from bfm_model.bfm.dataloader_helpers import get_val_dataloader\n",
    "\n",
    "hydra.initialize(config_path=\"../bfm_model/bfm/configs\", version_base=None)\n",
    "cfg = hydra.compose(config_name=\"train_config\", overrides=[\"model.embed_dim=256\", \"model.depth=3\",\n",
    "\"model.swin_backbone_size=medium\", \"model.num_heads=16\", \"training.devices=[0]\"]\n",
    "    )\n",
    "\n",
    "# print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "# LOAD THE MODEL \n",
    "checkpoint_repo = \"BioDT/bfm-pretrained\"\n",
    "dcheckpoint_name = \"bfm-pretrained-small.safetensors\"\n",
    "checkpoint_path = hf_hub_download(repo_id=checkpoint_repo, filename=dcheckpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = LargeClimateDataset(\n",
    "    data_dir=cfg.data.test_data_path, # Adapt that to your folder that contains the batches\n",
    "    scaling_settings=cfg.data.scaling,\n",
    "    num_species=cfg.data.species_number,\n",
    "    atmos_levels=cfg.data.atmos_levels,\n",
    "    model_patch_size=cfg.model.patch_size,\n",
    ")\n",
    "# override batch_size\n",
    "test_dataloader = get_val_dataloader(cfg, batch_size_override=cfg.evaluation.batch_size)\n",
    "\n",
    "\n",
    "bfm_model = setup_bfm_model(cfg, mode=\"test\")\n",
    "\n",
    "# When you load from HF\n",
    "state_dict_path = load_file(checkpoint_path)\n",
    "\n",
    "# When you have a local checkpoint path comment the HF path and add your local state_dict_path = \"path\"\n",
    "bfm_model.load_state_dict(state_dict_path, strict=False)\n",
    "\n",
    "\n",
    "bfm_model.eval()\n",
    "bfm_model.to(\"cuda\")\n",
    "\n",
    "trainer = get_trainer(\n",
    "    cfg,\n",
    "    mlflow_logger=None,\n",
    "    callbacks=[],\n",
    ")\n",
    "\n",
    "predictions = trainer.predict(model=bfm_model, dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_prediction = predictions[-1][0][\"pred\"]\n",
    "last_ground_truth = predictions[-1][0][\"gt\"]\n",
    "\n",
    "last_ground_truth.batch_metadata.timestamp\n",
    "# Thus the prediction is 1 month ahead -> 2020-07-01 00:00:00"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
